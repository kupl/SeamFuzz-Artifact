diff --git a/analysis/benchmark_results.py b/analysis/benchmark_results.py
index 178723f..39ed41c 100644
--- a/analysis/benchmark_results.py
+++ b/analysis/benchmark_results.py
@@ -95,30 +95,30 @@ class BenchmarkResults:
     @property
     @functools.lru_cache()
     def _benchmark_coverage_dict(self):
-        """Covered branches of each fuzzer on this benchmark."""
+        """Covered regions of each fuzzer on this benchmark."""
         return coverage_data_utils.get_benchmark_cov_dict(
             self._coverage_dict, self.name)
 
     @property
     @functools.lru_cache()
     def _benchmark_aggregated_coverage_df(self):
-        """Aggregated covered branches of each fuzzer on this benchmark."""
+        """Aggregated covered regions of each fuzzer on this benchmark."""
         return coverage_data_utils.get_benchmark_aggregated_cov_df(
             self._coverage_dict, self.name)
 
     @property
     @functools.lru_cache()
-    def _unique_branch_dict(self):
-        """Unique branches with the fuzzers that cover it."""
-        return coverage_data_utils.get_unique_branch_dict(
+    def _unique_region_dict(self):
+        """Unique regions with the fuzzers that cover it."""
+        return coverage_data_utils.get_unique_region_dict(
             self._benchmark_coverage_dict)
 
     @property
     @functools.lru_cache()
-    def unique_branch_cov_df(self):
-        """Fuzzers with the number of covered unique branches."""
-        return coverage_data_utils.get_unique_branch_cov_df(
-            self._unique_branch_dict, self.fuzzer_names)
+    def unique_region_cov_df(self):
+        """Fuzzers with the number of covered unique regions."""
+        return coverage_data_utils.get_unique_region_cov_df(
+            self._unique_region_dict, self.fuzzer_names)
 
     @property
     def fuzzers_with_not_enough_samples(self):
@@ -344,12 +344,12 @@ class BenchmarkResults:
 
     @property
     def violin_plot(self):
-        """Branch coverage violin plot."""
+        """Region coverage violin plot."""
         return self._generic_violin_plot('violin.svg')
 
     @property
     def bug_violin_plot(self):
-        """Branch coverage violin plot."""
+        """Region coverage violin plot."""
         return self._generic_violin_plot('bug_violin.svg', bugs=True)
 
     def _generic_box_plot(self, filename, bugs=False):
@@ -362,7 +362,7 @@ class BenchmarkResults:
 
     @property
     def box_plot(self):
-        """Branch coverage boxplot."""
+        """Region coverage boxplot."""
         return self._generic_box_plot('boxplot.svg')
 
     @property
@@ -399,19 +399,19 @@ class BenchmarkResults:
     @property
     def unique_coverage_ranking_plot(self):
         """Ranking plot for unique coverage."""
-        plot_filename = self._prefix_with_benchmark('ranking_unique_branch.svg')
-        unique_branch_cov_df_combined = self.unique_branch_cov_df.merge(
+        plot_filename = self._prefix_with_benchmark('ranking_unique_region.svg')
+        unique_region_cov_df_combined = self.unique_region_cov_df.merge(
             self._benchmark_aggregated_coverage_df, on='fuzzer')
         self._plotter.write_unique_coverage_ranking_plot(
-            unique_branch_cov_df_combined, self._get_full_path(plot_filename))
+            unique_region_cov_df_combined, self._get_full_path(plot_filename))
         return plot_filename
 
     @property
     @functools.lru_cache()
     def pairwise_unique_coverage_table(self):
         """Pairwise unique coverage table for each pair of fuzzers."""
-        fuzzers = self.unique_branch_cov_df.sort_values(
-            by='unique_branches_covered', ascending=False).fuzzer
+        fuzzers = self.unique_region_cov_df.sort_values(
+            by='unique_regions_covered', ascending=False).fuzzer
         return coverage_data_utils.get_pairwise_unique_coverage_table(
             self._benchmark_coverage_dict, fuzzers)
 
diff --git a/analysis/coverage_data_utils.py b/analysis/coverage_data_utils.py
index 90cb22d..b575409 100644
--- a/analysis/coverage_data_utils.py
+++ b/analysis/coverage_data_utils.py
@@ -74,96 +74,96 @@ def get_coverage_report_filestore_path(fuzzer: str, benchmark: str,
                           fuzzer, 'index.html')
 
 
-def get_covered_branches_dict(experiment_df: pd.DataFrame) -> Dict:
+def get_covered_regions_dict(experiment_df: pd.DataFrame) -> Dict:
     """Combines json files for different fuzzer-benchmark pair in
-    |experiment_df| and returns a dictionary of the covered branches."""
+    |experiment_df| and returns a dictionary of the covered regions."""
     fuzzers_and_benchmarks = set(
         zip(experiment_df.fuzzer, experiment_df.benchmark))
     arguments = [(fuzzer, benchmark,
                   get_experiment_filestore_path_for_fuzzer_benchmark(
                       fuzzer, benchmark, experiment_df))
                  for fuzzer, benchmark in fuzzers_and_benchmarks]
-    result = itertools.starmap(get_fuzzer_benchmark_covered_branches_and_key,
+    result = itertools.starmap(get_fuzzer_benchmark_covered_regions_and_key,
                                arguments)
     return dict(result)
 
 
-def get_fuzzer_benchmark_covered_branches_filestore_path(
+def get_fuzzer_benchmark_covered_regions_filestore_path(
         fuzzer: str, benchmark: str, exp_filestore_path: str) -> str:
-    """Returns the path to the covered branches json file in the |filestore| for
+    """Returns the path to the covered regions json file in the |filestore| for
     |fuzzer| and |benchmark|."""
     return posixpath.join(exp_filestore_path, 'coverage', 'data', benchmark,
-                          fuzzer, 'covered_branches.json')
+                          fuzzer, 'covered_regions.json')
 
 
-def get_fuzzer_covered_branches(fuzzer: str, benchmark: str, filestore: str):
-    """Returns the covered branches dict for |fuzzer| from the json file in the
+def get_fuzzer_covered_regions(fuzzer: str, benchmark: str, filestore: str):
+    """Returns the covered regions dict for |fuzzer| from the json file in the
     filestore."""
-    src_file = get_fuzzer_benchmark_covered_branches_filestore_path(
+    src_file = get_fuzzer_benchmark_covered_regions_filestore_path(
         fuzzer, benchmark, filestore)
     with tempfile.NamedTemporaryFile() as dst_file:
         if filestore_utils.cp(src_file, dst_file.name,
                               expect_zero=False).retcode:
-            logger.warning(
-                'covered_branches.json file: %s could not be copied.', src_file)
+            logger.warning('covered_regions.json file: %s could not be copied.',
+                           src_file)
             return {}
         with open(dst_file.name) as json_file:
             return json.load(json_file)
 
 
-def get_fuzzer_benchmark_covered_branches_and_key(
+def get_fuzzer_benchmark_covered_regions_and_key(
         fuzzer: str, benchmark: str, filestore: str) -> Tuple[str, Dict]:
     """Accepts |fuzzer|, |benchmark|, |filestore|.
-    Returns a tuple containing the fuzzer benchmark key and the branches covered
+    Returns a tuple containing the fuzzer benchmark key and the regions covered
     by the fuzzer on the benchmark."""
-    fuzzer_benchmark_covered_branches = get_fuzzer_covered_branches(
+    fuzzer_benchmark_covered_regions = get_fuzzer_covered_regions(
         fuzzer, benchmark, filestore)
     key = fuzzer_and_benchmark_to_key(fuzzer, benchmark)
-    return key, fuzzer_benchmark_covered_branches
+    return key, fuzzer_benchmark_covered_regions
 
 
-def get_unique_branch_dict(benchmark_coverage_dict: Dict) -> Dict:
+def get_unique_region_dict(benchmark_coverage_dict: Dict) -> Dict:
     """Returns a dictionary containing the covering fuzzers for each unique
-    branch, where the |threshold| defines which branches are unique."""
-    branch_dict = collections.defaultdict(list)
-    unique_branch_dict = {}
+    region, where the |threshold| defines which regions are unique."""
+    region_dict = collections.defaultdict(list)
+    unique_region_dict = {}
     threshold_count = 1
     for fuzzer in benchmark_coverage_dict:
-        for branch in benchmark_coverage_dict[fuzzer]:
-            branch_dict[branch].append(fuzzer)
-    for branch, fuzzers in branch_dict.items():
+        for region in benchmark_coverage_dict[fuzzer]:
+            region_dict[region].append(fuzzer)
+    for region, fuzzers in region_dict.items():
         if len(fuzzers) <= threshold_count:
-            unique_branch_dict[branch] = fuzzers
-    return unique_branch_dict
+            unique_region_dict[region] = fuzzers
+    return unique_region_dict
 
 
-def get_unique_branch_cov_df(unique_branch_dict: Dict,
+def get_unique_region_cov_df(unique_region_dict: Dict,
                              fuzzer_names: List[str]) -> pd.DataFrame:
     """Returns a DataFrame where the two columns are fuzzers and the number of
-    unique branches covered."""
+    unique regions covered."""
     fuzzers = collections.defaultdict(int)
-    for branch in unique_branch_dict:
-        for fuzzer in unique_branch_dict[branch]:
+    for region in unique_region_dict:
+        for fuzzer in unique_region_dict[region]:
             fuzzers[fuzzer] += 1
-    dict_to_transform = {'fuzzer': [], 'unique_branches_covered': []}
+    dict_to_transform = {'fuzzer': [], 'unique_regions_covered': []}
     for fuzzer in fuzzer_names:
         covered_num = fuzzers[fuzzer]
         dict_to_transform['fuzzer'].append(fuzzer)
-        dict_to_transform['unique_branches_covered'].append(covered_num)
+        dict_to_transform['unique_regions_covered'].append(covered_num)
     return pd.DataFrame(dict_to_transform)
 
 
 def get_benchmark_cov_dict(coverage_dict, benchmark):
-    """Returns a dictionary to store the covered branches of each fuzzer. Uses a
-    set of tuples to store the covered branches."""
+    """Returns a dictionary to store the covered regions of each fuzzer. Uses a
+    set of tuples to store the covered regions."""
     benchmark_cov_dict = {}
-    for key, covered_braches in coverage_dict.items():
+    for key, covered_regions in coverage_dict.items():
         current_fuzzer, current_benchmark = key_to_fuzzer_and_benchmark(key)
         if current_benchmark == benchmark:
-            covered_braches_in_set = set()
-            for branch in covered_braches:
-                covered_braches_in_set.add(tuple(branch))
-            benchmark_cov_dict[current_fuzzer] = covered_braches_in_set
+            covered_regions_in_set = set()
+            for region in covered_regions:
+                covered_regions_in_set.add(tuple(region))
+            benchmark_cov_dict[current_fuzzer] = covered_regions_in_set
     return benchmark_cov_dict
 
 
@@ -171,12 +171,12 @@ def get_benchmark_aggregated_cov_df(coverage_dict, benchmark):
     """Returns a dataframe where each row represents a fuzzer and its aggregated
     coverage number."""
     dict_to_transform = {'fuzzer': [], 'aggregated_edges_covered': []}
-    for key, covered_branches in coverage_dict.items():
+    for key, covered_regions in coverage_dict.items():
         current_fuzzer, current_benchmark = key_to_fuzzer_and_benchmark(key)
         if current_benchmark == benchmark:
             dict_to_transform['fuzzer'].append(current_fuzzer)
             dict_to_transform['aggregated_edges_covered'].append(
-                len(covered_branches))
+                len(covered_regions))
     return pd.DataFrame(dict_to_transform)
 
 
@@ -186,7 +186,7 @@ def get_pairwise_unique_coverage_table(benchmark_coverage_dict, fuzzers):
 
     The pairwise unique coverage table is a square matrix where each
     row and column represents a fuzzer, and each cell contains a number
-    showing the branches covered by the fuzzer of the column but not by
+    showing the regions covered by the fuzzer of the column but not by
     the fuzzer of the row."""
 
     pairwise_unique_coverage_values = []
@@ -204,16 +204,16 @@ def get_pairwise_unique_coverage_table(benchmark_coverage_dict, fuzzers):
                         columns=fuzzers)
 
 
-def get_unique_covered_percentage(fuzzer_row_covered_branches,
-                                  fuzzer_col_covered_branches):
-    """Returns the number of branches covered by the fuzzer of the
-    column but not by the fuzzer of the row."""
+def get_unique_covered_percentage(fuzzer_row_covered_regions,
+                                  fuzzer_col_covered_regions):
+    """Returns the number of regions covered by the fuzzer of the column but not
+    by the fuzzer of the row."""
 
-    unique_branch_count = 0
-    for branch in fuzzer_col_covered_branches:
-        if branch not in fuzzer_row_covered_branches:
-            unique_branch_count += 1
-    return unique_branch_count
+    unique_region_count = 0
+    for region in fuzzer_col_covered_regions:
+        if region not in fuzzer_row_covered_regions:
+            unique_region_count += 1
+    return unique_region_count
 
 
 def rank_by_average_normalized_score(benchmarks_unique_coverage_list):
diff --git a/analysis/experiment_results.py b/analysis/experiment_results.py
index b7be0ae..d3f46d8 100644
--- a/analysis/experiment_results.py
+++ b/analysis/experiment_results.py
@@ -24,14 +24,6 @@ from analysis import data_utils
 from analysis import stat_tests
 
 
-def strip_gs_protocol(url):
-    """Removes the leading gs:// from |url|."""
-    protocol = 'gs://'
-    if url.startswith(protocol):
-        return url[len(protocol):]
-    return url
-
-
 class ExperimentResults:  # pylint: disable=too-many-instance-attributes
     """Provides the main interface for getting various analysis results and
     plots about an experiment, represented by |experiment_df|.
@@ -94,9 +86,6 @@ class ExperimentResults:  # pylint: disable=too-many-instance-attributes
         # Dictionary to store the full coverage data.
         self._coverage_dict = coverage_dict
 
-        self.experiment_filestore = strip_gs_protocol(
-            experiment_df.experiment_filestore.iloc[0])
-
     def _get_full_path(self, filename):
         return os.path.join(self._output_directory, filename)
 
@@ -264,7 +253,7 @@ class ExperimentResults:  # pylint: disable=too-many-instance-attributes
         """Rank fuzzers using average normalized score on unique code coverage
         across benchmarks."""
         benchmarks_unique_coverage_list = [
-            benchmark.unique_branch_cov_df for benchmark in self.benchmarks
+            benchmark.unique_region_cov_df for benchmark in self.benchmarks
         ]
         return coverage_data_utils.rank_by_average_normalized_score(
             benchmarks_unique_coverage_list)
diff --git a/analysis/generate_report.py b/analysis/generate_report.py
index 06ad9ee..efbe4ef 100644
--- a/analysis/generate_report.py
+++ b/analysis/generate_report.py
@@ -226,7 +226,7 @@ def generate_report(experiment_names,
     coverage_dict = {}
     if coverage_report:
         logger.info('Generating coverage report info.')
-        coverage_dict = coverage_data_utils.get_covered_branches_dict(
+        coverage_dict = coverage_data_utils.get_covered_regions_dict(
             experiment_df)
         logger.info('Finished generating coverage report info.')
 
diff --git a/analysis/plotting.py b/analysis/plotting.py
index 14b27d5..ad57841 100644
--- a/analysis/plotting.py
+++ b/analysis/plotting.py
@@ -181,7 +181,7 @@ class Plotter:
                     loc='upper left',
                     frameon=False)
 
-        axes.set(ylabel='Bug coverage' if bugs else 'Code branch coverage')
+        axes.set(ylabel='Bug coverage' if bugs else 'Code region coverage')
         axes.set(xlabel='Time (hour:minute)')
 
         if self._logscale or logscale:
@@ -260,7 +260,7 @@ class Plotter:
             sns.stripplot(**common_args, size=3, color="black", alpha=0.6)
 
         axes.set_title(_formatted_title(benchmark_snapshot_df))
-        ylabel = 'Reached {} coverage'.format('bug' if bugs else 'branch')
+        ylabel = 'Reached {} coverage'.format('bug' if bugs else 'region')
         axes.set(ylabel=ylabel)
         axes.set(xlabel='Fuzzer (highest median coverage on the left)')
         axes.set_xticklabels(axes.get_xticklabels(),
@@ -307,7 +307,7 @@ class Plotter:
         axes.set_title(_formatted_title(benchmark_snapshot_df))
         axes.legend(loc='upper right', frameon=False)
 
-        axes.set(xlabel='Bug coverage' if bugs else 'Code branch coverage')
+        axes.set(xlabel='Bug coverage' if bugs else 'Code region coverage')
         axes.set(ylabel='Density')
         axes.set_xticklabels(axes.get_xticklabels(),
                              rotation=_DEFAULT_LABEL_ROTATION,
@@ -339,7 +339,7 @@ class Plotter:
                            ax=axes)
 
         axes.set_title(_formatted_title(benchmark_snapshot_df))
-        ylabel = 'Reached {} coverage'.format('bug' if bugs else 'branch')
+        ylabel = 'Reached {} coverage'.format('bug' if bugs else 'region')
         axes.set(ylabel=ylabel)
         axes.set(xlabel='Fuzzer (highest median coverage on the left)')
         axes.set_xticklabels(axes.get_xticklabels(),
@@ -413,8 +413,7 @@ class Plotter:
         cmap_colors = ['#005a32', '#238b45', '#a1d99b', '#fbd7d4']
         cmap = colors.ListedColormap(cmap_colors)
 
-        # TODO(lszekeres): Add 1 back to this list.
-        boundaries = [0, 0.001, 0.01, 0.05]
+        boundaries = [0, 0.001, 0.01, 0.05, 1]
         norm = colors.BoundaryNorm(boundaries, cmap.N)
 
         if symmetric:
@@ -491,17 +490,17 @@ class Plotter:
             plt.close(fig)
 
     def unique_coverage_ranking_plot(self,
-                                     unique_branch_cov_df_combined,
+                                     unique_region_cov_df_combined,
                                      axes=None):
         """Draws unique_coverage_ranking plot. The fuzzer labels will be in
         the order of their coverage."""
 
-        fuzzer_order = unique_branch_cov_df_combined.sort_values(
-            by='unique_branches_covered', ascending=False).fuzzer
+        fuzzer_order = unique_region_cov_df_combined.sort_values(
+            by='unique_regions_covered', ascending=False).fuzzer
 
-        axes = sns.barplot(y='unique_branches_covered',
+        axes = sns.barplot(y='unique_regions_covered',
                            x='fuzzer',
-                           data=unique_branch_cov_df_combined,
+                           data=unique_region_cov_df_combined,
                            order=fuzzer_order,
                            palette=self._fuzzer_colors,
                            ax=axes)
@@ -517,7 +516,7 @@ class Plotter:
 
         sns.barplot(y='aggregated_edges_covered',
                     x='fuzzer',
-                    data=unique_branch_cov_df_combined,
+                    data=unique_region_cov_df_combined,
                     order=fuzzer_order,
                     facecolor=(1, 1, 1, 0),
                     edgecolor='0.2',
@@ -531,11 +530,11 @@ class Plotter:
 
         sns.despine(ax=axes, trim=True)
 
-    def write_unique_coverage_ranking_plot(self, unique_branch_cov_df_combined,
+    def write_unique_coverage_ranking_plot(self, unique_region_cov_df_combined,
                                            image_path):
         """Writes ranking plot for unique coverage."""
         self._write_plot_to_image(self.unique_coverage_ranking_plot,
-                                  unique_branch_cov_df_combined,
+                                  unique_region_cov_df_combined,
                                   image_path,
                                   wide=True)
 
diff --git a/analysis/rendering.py b/analysis/rendering.py
index 94122c9..d6a2786 100644
--- a/analysis/rendering.py
+++ b/analysis/rendering.py
@@ -17,6 +17,7 @@ import os
 
 import jinja2
 
+from common import experiment_utils
 from common import utils
 
 
@@ -42,7 +43,8 @@ def render_report(experiment_results, template, in_progress, coverage_report,
     )
     template = environment.get_template(template)
 
-    config_path = 'input/config/experiment.yaml'
+    config_path = (
+        experiment_utils.get_internal_experiment_config_relative_path())
     return template.render(experiment=experiment_results,
                            in_progress=in_progress,
                            coverage_report=coverage_report,
diff --git a/analysis/report_templates/default.html b/analysis/report_templates/default.html
index 2d92525..9dc7e65 100644
--- a/analysis/report_templates/default.html
+++ b/analysis/report_templates/default.html
@@ -385,12 +385,12 @@
 
                         <div class="row">
                             <div class="col s6 offset-s3">
-                                <h5 class="center-align">Ranking by unique code branches covered</h4>
+                                <h5 class="center-align">Ranking by unique code regions covered</h4>
                                     <img class="responsive-img materialboxed"
                                          src="{{ benchmark.unique_coverage_ranking_plot }}">
-                                    Each bar shows the total number of code branches found by a given fuzzer.
-                                    The colored area shows the number of unique code branches
-                                    (i.e., branches that were not covered by any other fuzzers).
+                                    Each bar shows the total number of code regions found by a given fuzzer.
+                                    The colored area shows the number of unique code regions
+                                    (i.e., regions that were not covered by any other fuzzers).
                             </div>
                         </div> <!-- row -->
 
@@ -399,7 +399,7 @@
                                 <h5 class="center-align">Pairwise unique code coverage</h4>
                                     <img class="responsive-img materialboxed"
                                          src="{{ benchmark.pairwise_unique_coverage_plot }}">
-                                    Each cell represents the number of code branches covered by the fuzzer
+                                    Each cell represents the number of code regions covered by the fuzzer
                                     of the column but not by the fuzzer of the row
                             </div>
                         </div> <!-- row -->
@@ -446,7 +446,7 @@
             # Check out the right commit. <br>
             git checkout {{ experiment.git_hash }} <br>
             # Download the internal config file. <br>
-            curl https://storage.googleapis.com/{{ experiment.experiment_filestore}}/{{ experiment.name }}/{{ experiment_config_relative_path }} >  /tmp/experiment-config.yaml<br>
+            curl https://storage.googleapis.com/{{ experiment.name }}/{{ experiment_config_relative_path }} >  /tmp/experiment-config.yaml<br>
             make install-dependencies <br>
             # Launch the experiment using paramters from the internal config file. <br>
             PYTHONPATH=. python experiment/reproduce_experiment.py -c /tmp/experiment-config.yaml -e &lt;new_experiment_name&gt;
diff --git a/analysis/test_coverage_data_utils.py b/analysis/test_coverage_data_utils.py
index 18b2b1c..1a5127e 100644
--- a/analysis/test_coverage_data_utils.py
+++ b/analysis/test_coverage_data_utils.py
@@ -44,41 +44,41 @@ def create_coverage_data():
     }
 
 
-def test_get_unique_branch_dict():
-    """Tests get_unique_branch_dict() function."""
+def test_get_unique_region_dict():
+    """Tests get_unique_region_dict() function."""
     coverage_dict = create_coverage_data()
     benchmark_coverage_dict = coverage_data_utils.get_benchmark_cov_dict(
         coverage_dict, 'libpng-1.2.56')
-    unique_branch_dict = coverage_data_utils.get_unique_branch_dict(
+    unique_region_dict = coverage_data_utils.get_unique_region_dict(
         benchmark_coverage_dict)
     expected_dict = {
         (0, 0, 2, 2): ['afl'],
         (0, 0, 2, 3): ['libfuzzer'],
         (0, 0, 4, 4): ['libfuzzer']
     }
-    assert expected_dict == unique_branch_dict
+    assert expected_dict == unique_region_dict
 
 
-def test_get_unique_branch_cov_df():
-    """Tests get_unique_branch_cov_df() function."""
+def test_get_unique_region_cov_df():
+    """Tests get_unique_region_cov_df() function."""
     coverage_dict = create_coverage_data()
     benchmark_coverage_dict = coverage_data_utils.get_benchmark_cov_dict(
         coverage_dict, 'libpng-1.2.56')
-    unique_branch_dict = coverage_data_utils.get_unique_branch_dict(
+    unique_region_dict = coverage_data_utils.get_unique_region_dict(
         benchmark_coverage_dict)
     fuzzer_names = ['afl', 'libfuzzer']
-    unique_branch_df = coverage_data_utils.get_unique_branch_cov_df(
-        unique_branch_dict, fuzzer_names)
-    unique_branch_df = unique_branch_df.sort_values(by=['fuzzer']).reset_index(
+    unique_region_df = coverage_data_utils.get_unique_region_cov_df(
+        unique_region_dict, fuzzer_names)
+    unique_region_df = unique_region_df.sort_values(by=['fuzzer']).reset_index(
         drop=True)
     expected_df = pd.DataFrame([{
         'fuzzer': 'afl',
-        'unique_branches_covered': 1
+        'unique_regions_covered': 1
     }, {
         'fuzzer': 'libfuzzer',
-        'unique_branches_covered': 2
+        'unique_regions_covered': 2
     }])
-    assert unique_branch_df.equals(expected_df)
+    assert unique_region_df.equals(expected_df)
 
 
 def test_get_benchmark_cov_dict():
@@ -109,15 +109,15 @@ def test_get_pairwise_unique_coverage_table():
     pd_test.assert_frame_equal(table, expected_table)
 
 
-def test_get_fuzzer_benchmark_covered_branches_filestore_path():
-    """Tests that get_fuzzer_benchmark_covered_branches_filestore_path returns
+def test_get_fuzzer_benchmark_covered_regions_filestore_path():
+    """Tests that get_fuzzer_benchmark_covered_regions_filestore_path returns
     the correct result."""
-    assert (coverage_data_utils.
-            get_fuzzer_benchmark_covered_branches_filestore_path(
-                FUZZER, BENCHMARK, EXPERIMENT_FILESTORE_PATH) == (
-                    'gs://fuzzbench-data/myexperiment/'
-                    'coverage/data/libpng-1.2.56/afl/'
-                    'covered_branches.json'))
+    assert (
+        coverage_data_utils.get_fuzzer_benchmark_covered_regions_filestore_path(
+            FUZZER, BENCHMARK,
+            EXPERIMENT_FILESTORE_PATH) == ('gs://fuzzbench-data/myexperiment/'
+                                           'coverage/data/libpng-1.2.56/afl/'
+                                           'covered_regions.json'))
 
 
 def test_fuzzer_and_benchmark_to_key():
diff --git a/analysis/test_plotting.py b/analysis/test_plotting.py
index 0ec44a7..947c237 100644
--- a/analysis/test_plotting.py
+++ b/analysis/test_plotting.py
@@ -42,13 +42,13 @@ def test_unique_coverage_ranking_plot(tmp_path):
     fuzzer_num = 22
 
     fuzzers = [f'fuzzer-{i}' for i in range(fuzzer_num)]
-    unique_branchs = [10 * i for i in range(fuzzer_num)]
-    total_branches = [1000] * fuzzer_num
+    unique_regions = [10 * i for i in range(fuzzer_num)]
+    total_regions = [1000] * fuzzer_num
 
     df = pd.DataFrame({
         'fuzzer': fuzzers,
-        'unique_branches_covered': unique_branchs,
-        'aggregated_edges_covered': total_branches
+        'unique_regions_covered': unique_regions,
+        'aggregated_edges_covered': total_regions
     })
 
     plotter = plotting.Plotter(fuzzers)
diff --git a/common/benchmark_utils.py b/common/benchmark_utils.py
index 7eb3b1e..7f91f0b 100644
--- a/common/benchmark_utils.py
+++ b/common/benchmark_utils.py
@@ -162,19 +162,3 @@ def get_bug_benchmarks():
         benchmark for benchmark in get_all_benchmarks()
         if get_type(benchmark) == BenchmarkType.BUG.value
     ]
-
-
-def is_cpp(benchmark):
-    """Returns True if |benchmark| is written in C/C++."""
-    return get_language(benchmark) == 'c++'
-
-
-def exclude_non_cpp(benchmarks):
-    """Returns |benchmarks| with only benchmarks written in C/C++."""
-    return [benchmark for benchmark in benchmarks if is_cpp(benchmark)]
-
-
-def get_language(benchmark):
-    """Returns the prorgamming language the benchmark was written in."""
-    config = benchmark_config.get_config(benchmark)
-    return config.get('language', 'c++')
diff --git a/common/experiment_utils.py b/common/experiment_utils.py
index 2bbd648..c533d12 100644
--- a/common/experiment_utils.py
+++ b/common/experiment_utils.py
@@ -72,12 +72,6 @@ def get_oss_fuzz_corpora_filestore_path():
     return posixpath.join(get_experiment_filestore_path(), 'oss_fuzz_corpora')
 
 
-def get_custom_seed_corpora_filestore_path():
-    """Returns path containing the user-provided seed corpora."""
-    return posixpath.join(get_experiment_filestore_path(),
-                          'custom_seed_corpora')
-
-
 def get_dispatcher_instance_name(experiment: str) -> str:
     """Returns a dispatcher instance name for an experiment."""
     return 'd-%s' % experiment
diff --git a/common/fuzzer_config.py b/common/fuzzer_config.py
deleted file mode 100644
index 0d0b92c..0000000
--- a/common/fuzzer_config.py
+++ /dev/null
@@ -1,35 +0,0 @@
-# Copyright 2022 Google LLC
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-"""Tools for using oss-fuzz."""
-import functools
-import os
-
-from common import utils
-from common import yaml_utils
-
-FUZZERS_DIR = os.path.join(utils.ROOT_DIR, 'fuzzers')
-
-
-def get_config_file(fuzzer):
-    """Returns the path to the config for a fuzzer."""
-    return os.path.join(FUZZERS_DIR, fuzzer, 'fuzzer.yaml')
-
-
-@functools.lru_cache(maxsize=None)
-def get_config(fuzzer):
-    """Returns a dictionary containing the config for a fuzzer."""
-    config_file = get_config_file(fuzzer)
-    if os.path.exists(config_file):
-        return yaml_utils.read(config_file)
-    return {}
diff --git a/common/fuzzer_utils.py b/common/fuzzer_utils.py
index 3d5cd84..506ca14 100644
--- a/common/fuzzer_utils.py
+++ b/common/fuzzer_utils.py
@@ -18,7 +18,6 @@ import os
 import re
 from typing import Optional
 
-from common import fuzzer_config
 from common import logs
 from common import utils
 
@@ -140,9 +139,3 @@ def get_fuzzer_names():
         fuzzers.append(fuzzer)
 
     return fuzzers
-
-
-def get_languages(fuzzer):
-    """Returns the programming languages |fuzzer| can fuzz."""
-    config = fuzzer_config.get_config(fuzzer)
-    return config.get('languages', ['c++'])
diff --git a/common/utils.py b/common/utils.py
index 3b508f2..6aa1d1e 100644
--- a/common/utils.py
+++ b/common/utils.py
@@ -14,7 +14,6 @@
 """Common utilities."""
 
 import hashlib
-import http.client
 import os
 import urllib.request
 import urllib.error
@@ -51,8 +50,6 @@ def is_local():
         _is_local = False
     except urllib.error.URLError:
         _is_local = True
-    except http.client.RemoteDisconnected:
-        _is_local = True
     return _is_local
 
 
diff --git a/common/yaml_utils.py b/common/yaml_utils.py
index 54191ec..2e3a285 100644
--- a/common/yaml_utils.py
+++ b/common/yaml_utils.py
@@ -12,11 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Yaml helpers."""
+import os
 import yaml
 
 
 def read(yaml_filename):
     """Reads and loads yaml file specified by |yaml_filename|."""
+    if not os.path.exists(yaml_filename):
+        raise Exception('Yaml file %s does not exist.' % yaml_filename)
+
     with open(yaml_filename) as file_handle:
         return yaml.load(file_handle, yaml.SafeLoader)
 
diff --git a/docs/Gemfile.lock b/docs/Gemfile.lock
index ff96453..4708f48 100644
--- a/docs/Gemfile.lock
+++ b/docs/Gemfile.lock
@@ -225,7 +225,7 @@ GEM
       jekyll-seo-tag (~> 2.1)
     minitest (5.14.4)
     multipart-post (2.1.1)
-    nokogiri (1.13.4-x86_64-linux)
+    nokogiri (1.12.2-x86_64-linux)
       racc (~> 1.4)
     octokit (4.21.0)
       faraday (>= 0.9)
@@ -233,7 +233,7 @@ GEM
     pathutil (0.16.2)
       forwardable-extended (~> 2.6)
     public_suffix (4.0.6)
-    racc (1.6.0)
+    racc (1.5.2)
     rb-fsevent (0.11.0)
     rb-inotify (0.10.1)
       ffi (~> 1.0)
diff --git a/docs/developing-fuzzbench/adding_a_new_benchmark.md b/docs/developing-fuzzbench/adding_a_new_benchmark.md
index bf10478..a5479c4 100755
--- a/docs/developing-fuzzbench/adding_a_new_benchmark.md
+++ b/docs/developing-fuzzbench/adding_a_new_benchmark.md
@@ -215,9 +215,19 @@ make build-$FUZZER_NAME-$BENCHMARK_NAME
 # This command will fuzz forever. Press Ctrl-C to stop it.
 make run-$FUZZER_NAME-$BENCHMARK_NAME
 ```
+Finally, add your benchmark to the list of OSS-Fuzz benchmarks in
+[test_fuzzer_benchmarks.py](https://github.com/google/fuzzbench/blob/master/.github/workflows/test_fuzzer_benchmarks.py)
+
+This ensures that CI tests your benchmark with all fuzzers.
 
 ## Submitting the benchmark in a pull request
 
+Add your benchmark to a list in
+[build_and_test_run_fuzzer_benchmarks.py](https://github.com/google/fuzzbench/blob/master/.github/workflows/build_and_test_run_fuzzer_benchmarks.py)
+so that it will be tested in CI. If your benchmark is a standard benchmark, add
+it to the `STANDARD_BENCHMARKS` list, otherwise add it to the the
+`OSS_FUZZ_BENCHMARKS` list.
+
 If everything works, submit the integration in a
 [GitHub pull request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request).
 
diff --git a/docs/getting-started/adding_a_new_fuzzer.md b/docs/getting-started/adding_a_new_fuzzer.md
index ecf416b..abee9d8 100644
--- a/docs/getting-started/adding_a_new_fuzzer.md
+++ b/docs/getting-started/adding_a_new_fuzzer.md
@@ -231,13 +231,6 @@ export BENCHMARK_NAME=libpng-1.2.56
 make build-$FUZZER_NAME-$BENCHMARK_NAME
 ```
 
-* To debug a build:
-
-```shell
-make debug-builder-$FUZZER_NAME-$BENCHMARK_NAME
-```
-And then run `fuzzer_build` when you have a shell on the builder.
-
 * Run the fuzzer in the docker image:
 
 ```shell
diff --git a/docs/running-a-local-experiment/running_a_local_experiment.md b/docs/running-a-local-experiment/running_a_local_experiment.md
index 70c6d2a..9306fef 100644
--- a/docs/running-a-local-experiment/running_a_local_experiment.md
+++ b/docs/running-a-local-experiment/running_a_local_experiment.md
@@ -92,12 +92,6 @@ You can optionally add:
 * `--no-dictionaries` - to skip using dictionaries across all benchmarks.
 * `--oss-fuzz-corpus` - use the latest corpora from OSS-Fuzz across all
   benchmarks (where available).
-* `--concurrent-builds N` - to limit the number of concurrent builds, useful
-  when having limited memory.
-* `--runners-cpus` - to limit the number of usable CPUs by the runner containers
-  (in which fuzzers run).
-* `--measurers-cpus` - to limit the number of usable CPUs by the measurer
-  containers.
 
 ## Viewing reports
 
diff --git a/experiment/build/builder.py b/experiment/build/builder.py
index d717760..706992c 100644
--- a/experiment/build/builder.py
+++ b/experiment/build/builder.py
@@ -25,10 +25,7 @@ import time
 from typing import Callable, List, Tuple
 
 from common import benchmark_config
-from common import benchmark_utils
 from common import experiment_utils
-from common import fuzzer_config
-from common import fuzzer_utils
 from common import filesystem
 from common import utils
 from common import logs
@@ -58,35 +55,17 @@ def get_fuzzer_benchmark_pairs(fuzzers, benchmarks):
     """Return a tuple of (fuzzer, benchmark) pairs to build. Excludes
     unsupported fuzzers from each benchmark.
     """
-    unsupported_fuzzer_benchmark_pairs = set()
-    for fuzzer in fuzzers:
-        config = fuzzer_config.get_config(fuzzer)
-        unsupported_benchmarks = config.get('unsupported_benchmarks')
-        if unsupported_benchmarks:
-            unsupported_fuzzer_benchmark_pairs.update(
-                itertools.product([fuzzer], unsupported_benchmarks))
-        allowed_benchmarks = config.get('allowed_benchmarks')
-        if allowed_benchmarks:
-            for benchmark in benchmarks:
-                if benchmark not in allowed_benchmarks:
-                    unsupported_fuzzer_benchmark_pairs.add((fuzzer, benchmark))
-
-        # Handle languages.
-        fuzzer_languages = fuzzer_utils.get_languages(fuzzer)
-        for benchmark in benchmarks:
-            benchmark_language = benchmark_utils.get_language(benchmark)
-            if benchmark_language not in fuzzer_languages:
-                unsupported_fuzzer_benchmark_pairs.add((fuzzer, benchmark))
+    fuzzer_benchmark_pairs = list(itertools.product(fuzzers, benchmarks))
 
+    unsupported_fuzzer_benchmark_pairs = []
     for benchmark in benchmarks:
         config = benchmark_config.get_config(benchmark)
         unsupported_fuzzers = config.get('unsupported_fuzzers')
         if not unsupported_fuzzers:
             continue
-        unsupported_fuzzer_benchmark_pairs.update(
+        unsupported_fuzzer_benchmark_pairs += list(
             itertools.product(unsupported_fuzzers, [benchmark]))
 
-    fuzzer_benchmark_pairs = list(itertools.product(fuzzers, benchmarks))
     return [
         i for i in fuzzer_benchmark_pairs
         if i not in unsupported_fuzzer_benchmark_pairs
diff --git a/experiment/build/gcb_build.py b/experiment/build/gcb_build.py
index 32f1187..b6e1493 100644
--- a/experiment/build/gcb_build.py
+++ b/experiment/build/gcb_build.py
@@ -25,6 +25,11 @@ from experiment.build import build_utils
 from experiment.build import docker_images
 from experiment.build import generate_cloudbuild
 
+BUILDER_STEP_IDS = [
+    'build-fuzzer-builder',
+    'build-fuzzer-benchmark-builder',
+    'build-fuzzer-benchmark-builder-intermediate',
+]
 CONFIG_DIR = 'config'
 
 # Maximum time to wait for a GCB config to finish build.
diff --git a/experiment/dispatcher.py b/experiment/dispatcher.py
index c14d50c..db883cb 100755
--- a/experiment/dispatcher.py
+++ b/experiment/dispatcher.py
@@ -41,8 +41,8 @@ LOOP_WAIT_SECONDS = 5 * 60
 # TODO(metzman): Convert more uses of os.path.join to exp_path.path.
 
 
-def _get_config_file_path():
-    """Return config file path."""
+def _get_config_path():
+    """Return config directory."""
     return exp_path.path(
         experiment_utils.get_internal_experiment_config_relative_path())
 
@@ -100,6 +100,7 @@ class Experiment:  # pylint: disable=too-many-instance-attributes
         self.num_trials = self.config['trials']
         self.experiment_name = self.config['experiment']
         self.git_hash = self.config['git_hash']
+        self.concurrent_builds = self.config['concurrent_builds']
         self.preemptible = self.config.get('preemptible_runners')
 
 
@@ -148,7 +149,7 @@ def dispatcher_main():
     if experiment_utils.is_local_experiment():
         models.Base.metadata.create_all(db_utils.engine)
 
-    experiment_config_file_path = _get_config_file_path()
+    experiment_config_file_path = _get_config_path()
     experiment = Experiment(experiment_config_file_path)
 
     _initialize_experiment_in_db(experiment.config)
@@ -156,7 +157,7 @@ def dispatcher_main():
     trials = build_images_for_trials(experiment.fuzzers, experiment.benchmarks,
                                      experiment.num_trials,
                                      experiment.preemptible,
-                                     experiment.config['concurrent_builds'])
+                                     experiment.concurrent_builds)
     _initialize_trials_in_db(trials)
 
     create_work_subdirs(['experiment-folders', 'measurement-folders'])
@@ -204,12 +205,12 @@ def main():
     except Exception as error:
         logs.error('Error conducting experiment.')
         raise error
+    experiment_config_file_path = os.path.join(_get_config_path(),
+                                               'experiment.yaml')
 
     if experiment_utils.is_local_experiment():
         return 0
 
-    experiment_config_file_path = _get_config_file_path()
-
     if stop_experiment.stop_experiment(experiment_utils.get_experiment_name(),
                                        experiment_config_file_path):
         return 0
diff --git a/experiment/measurer/coverage_utils.py b/experiment/measurer/coverage_utils.py
index 7123ed4..0122b84 100644
--- a/experiment/measurer/coverage_utils.py
+++ b/experiment/measurer/coverage_utils.py
@@ -48,17 +48,15 @@ def generate_coverage_reports(experiment_config: dict):
     benchmarks = experiment_config['benchmarks']
     fuzzers = experiment_config['fuzzers']
     experiment = experiment_config['experiment']
-    region_coverage = experiment_config['region_coverage']
 
     for benchmark in benchmarks:
         for fuzzer in fuzzers:
-            generate_coverage_report(experiment, benchmark, fuzzer,
-                                     region_coverage)
+            generate_coverage_report(experiment, benchmark, fuzzer)
 
     logger.info('Finished generating coverage reports.')
 
 
-def generate_coverage_report(experiment, benchmark, fuzzer, region_coverage):
+def generate_coverage_report(experiment, benchmark, fuzzer):
     """Generates the coverage report for one pair of benchmark and fuzzer."""
     logger.info(
         ('Generating coverage report for '
@@ -66,8 +64,7 @@ def generate_coverage_report(experiment, benchmark, fuzzer, region_coverage):
                                                             fuzzer=fuzzer))
 
     try:
-        coverage_reporter = CoverageReporter(experiment, fuzzer, benchmark,
-                                             region_coverage)
+        coverage_reporter = CoverageReporter(experiment, fuzzer, benchmark)
 
         # Merges all the profdata files.
         coverage_reporter.merge_profdata_files()
@@ -75,8 +72,8 @@ def generate_coverage_report(experiment, benchmark, fuzzer, region_coverage):
         # Generate the coverage summary json file based on merged profdata file.
         coverage_reporter.generate_coverage_summary_json()
 
-        # Generate the coverage branches json file.
-        coverage_reporter.generate_coverage_branches_json()
+        # Generate the coverage regions json file.
+        coverage_reporter.generate_coverage_regions_json()
 
         # Generates the html reports using llvm-cov.
         coverage_reporter.generate_coverage_report()
@@ -91,12 +88,11 @@ class CoverageReporter:  # pylint: disable=too-many-instance-attributes
     fuzzer and benchmark."""
 
     # pylint: disable=too-many-arguments
-    def __init__(self, experiment, fuzzer, benchmark, region_coverage):
+    def __init__(self, experiment, fuzzer, benchmark):
         self.fuzzer = fuzzer
         self.benchmark = benchmark
         self.experiment = experiment
         self.trial_ids = get_trial_ids(experiment, fuzzer, benchmark)
-        self.region_coverage = region_coverage
 
         coverage_info_dir = get_coverage_info_dir()
         self.report_dir = os.path.join(coverage_info_dir, 'reports', benchmark,
@@ -172,19 +168,15 @@ class CoverageReporter:  # pylint: disable=too-many-instance-attributes
         dst_dir = exp_path.filestore(self.report_dir)
         filestore_utils.cp(src_dir, dst_dir, recursive=True, parallel=True)
 
-    def generate_coverage_branches_json(self):
+    def generate_coverage_regions_json(self):
         """Stores the coverage data in a json file."""
-        if self.region_coverage:
-            edges_covered = extract_covered_regions_from_summary_json(
-                self.merged_summary_json_file)
-        else:
-            edges_covered = extract_covered_branches_from_summary_json(
-                self.merged_summary_json_file)
-        coverage_json_src = os.path.join(self.data_dir, 'covered_branches.json')
+        covered_regions = extract_covered_regions_from_summary_json(
+            self.merged_summary_json_file)
+        coverage_json_src = os.path.join(self.data_dir, 'covered_regions.json')
         coverage_json_dst = exp_path.filestore(coverage_json_src)
         filesystem.create_directory(self.data_dir)
         with open(coverage_json_src, 'w') as file_handle:
-            json.dump(edges_covered, file_handle)
+            json.dump(covered_regions, file_handle)
         filestore_utils.cp(coverage_json_src,
                            coverage_json_dst,
                            expect_zero=False)
@@ -279,36 +271,6 @@ def generate_json_summary(coverage_binary,
     return result
 
 
-def extract_covered_branches_from_summary_json(summary_json_file):
-    """Returns the covered branches given a coverage summary json file."""
-    covered_branches = []
-    try:
-        coverage_info = get_coverage_infomation(summary_json_file)
-        functions_data = coverage_info['data'][0]['functions']
-
-        # The fourth and the fifth item tell whether the branch is evaluated to
-        # true or false respectively.
-        hit_true_index = 4
-        hit_false_index = 5
-        # The last number in the branch-list indicates what type of the
-        # region it is; 'branch_region' is represented by number 4.
-        type_index = -1
-        branch_region_type = 4
-        # The number of index 6 represents the file number.
-        file_index = 6
-        for function_data in functions_data:
-            for branch in function_data['branches']:
-                if branch[hit_true_index] != 0 or branch[
-                        hit_false_index] != 0 and branch[
-                            type_index] == branch_region_type:
-                    covered_branches.append(branch[:hit_true_index] +
-                                            branch[file_index:])
-
-    except Exception:  # pylint: disable=broad-except
-        logger.error('Coverage summary json file defective or missing.')
-    return covered_branches
-
-
 def extract_covered_regions_from_summary_json(summary_json_file):
     """Returns the covered regions given a coverage summary json file."""
     covered_regions = []
diff --git a/experiment/measurer/measure_manager.py b/experiment/measurer/measure_manager.py
index b926c25..4b6f842 100644
--- a/experiment/measurer/measure_manager.py
+++ b/experiment/measurer/measure_manager.py
@@ -27,7 +27,6 @@ import tarfile
 import time
 from typing import List, Set
 import queue
-import psutil
 
 from sqlalchemy import func
 from sqlalchemy import orm
@@ -74,11 +73,7 @@ def measure_main(experiment_config):
     # Start the measure loop first.
     experiment = experiment_config['experiment']
     max_total_time = experiment_config['max_total_time']
-    measurers_cpus = experiment_config['measurers_cpus']
-    runners_cpus = experiment_config['runners_cpus']
-    region_coverage = experiment_config['region_coverage']
-    measure_loop(experiment, max_total_time, measurers_cpus, runners_cpus,
-                 region_coverage)
+    measure_loop(experiment, max_total_time)
 
     # Clean up resources.
     gc.collect()
@@ -89,36 +84,11 @@ def measure_main(experiment_config):
     logger.info('Finished measuring.')
 
 
-def _process_init(cores_queue):
-    """Cpu pin for each pool process"""
-    cpu = cores_queue.get()
-    if sys.platform == 'linux':
-        psutil.Process().cpu_affinity([cpu])
-
-
-def measure_loop(experiment: str,
-                 max_total_time: int,
-                 measurers_cpus=None,
-                 runners_cpus=None,
-                 region_coverage=False):
+def measure_loop(experiment: str, max_total_time: int):
     """Continuously measure trials for |experiment|."""
     logger.info('Start measure_loop.')
 
-    pool_args = ()
-    if measurers_cpus is not None and runners_cpus is not None:
-        local_experiment = experiment_utils.is_local_experiment()
-        if local_experiment:
-            cores_queue = multiprocessing.Queue()
-            logger.info('Scheduling measurers from core %d to %d.' %
-                        (runners_cpus, runners_cpus + measurers_cpus - 1))
-            for cpu in range(runners_cpus, runners_cpus + measurers_cpus):
-                cores_queue.put(cpu)
-            pool_args = (measurers_cpus, _process_init, (cores_queue,))
-        else:
-            pool_args = (measurers_cpus,)
-
-    with multiprocessing.Pool(
-            *pool_args) as pool, multiprocessing.Manager() as manager:
+    with multiprocessing.Pool() as pool, multiprocessing.Manager() as manager:
         set_up_coverage_binaries(pool, experiment)
         # Using Multiprocessing.Queue will fail with a complaint about
         # inheriting queue.
@@ -129,8 +99,7 @@ def measure_loop(experiment: str,
                 # races.
                 all_trials_ended = scheduler.all_trials_ended(experiment)
 
-                if not measure_all_trials(experiment, max_total_time, pool, q,
-                                          region_coverage):
+                if not measure_all_trials(experiment, max_total_time, pool, q):
                     # We didn't measure any trials.
                     if all_trials_ended:
                         # There are no trials producing snapshots to measure.
@@ -145,8 +114,7 @@ def measure_loop(experiment: str,
     logger.info('Finished measure loop.')
 
 
-def measure_all_trials(experiment: str, max_total_time: int, pool, q,
-                       region_coverage) -> bool:  # pylint: disable=invalid-name
+def measure_all_trials(experiment: str, max_total_time: int, pool, q) -> bool:  # pylint: disable=invalid-name
     """Get coverage data (with coverage runs) for all active trials. Note that
     this should not be called unless multiprocessing.set_start_method('spawn')
     was called first. Otherwise it will use fork which breaks logging."""
@@ -163,7 +131,7 @@ def measure_all_trials(experiment: str, max_total_time: int, pool, q,
         return False
 
     measure_trial_coverage_args = [
-        (unmeasured_snapshot, max_cycle, q, region_coverage)
+        (unmeasured_snapshot, max_cycle, q)
         for unmeasured_snapshot in unmeasured_snapshots
     ]
 
@@ -361,9 +329,8 @@ class SnapshotMeasurer(coverage_utils.TrialCoverage):  # pylint: disable=too-man
 
     UNIT_BLACKLIST = collections.defaultdict(set)
 
-    # pylint: disable=too-many-arguments
     def __init__(self, fuzzer: str, benchmark: str, trial_num: int,
-                 trial_logger: logs.Logger, region_coverage: bool):
+                 trial_logger: logs.Logger):
         super().__init__(fuzzer, benchmark, trial_num)
         self.logger = trial_logger
         self.corpus_dir = os.path.join(self.measurement_dir, 'corpus')
@@ -393,9 +360,6 @@ class SnapshotMeasurer(coverage_utils.TrialCoverage):  # pylint: disable=too-man
         self.cov_summary_file = os.path.join(self.report_dir,
                                              'cov_summary.json')
 
-        # Use region coverage as coverage metric instead of branch (default)
-        self.region_coverage = region_coverage
-
     def get_profraw_files(self):
         """Return generated profraw files."""
         return [
@@ -445,14 +409,11 @@ class SnapshotMeasurer(coverage_utils.TrialCoverage):  # pylint: disable=too-man
         try:
             coverage_info = coverage_utils.get_coverage_infomation(
                 self.cov_summary_file)
-            coverage_data = coverage_info['data'][0]
-            summary_data = coverage_data['totals']
-            if self.region_coverage:
-                code_coverage_data = summary_data['regions']
-            else:
-                code_coverage_data = summary_data['branches']
-            code_coverage = code_coverage_data['covered']
-            return code_coverage
+            coverage_data = coverage_info["data"][0]
+            summary_data = coverage_data["totals"]
+            regions_coverage_data = summary_data["regions"]
+            regions_covered = regions_coverage_data["covered"]
+            return regions_covered
         except Exception:  # pylint: disable=broad-except
             self.logger.error(
                 'Coverage summary json file defective or missing.')
@@ -624,8 +585,8 @@ def get_fuzzer_stats(stats_filestore_path):
 
 
 def measure_trial_coverage(  # pylint: disable=invalid-name
-        measure_req, max_cycle: int, q: multiprocessing.Queue,
-        region_coverage) -> models.Snapshot:
+        measure_req, max_cycle: int,
+        q: multiprocessing.Queue) -> models.Snapshot:
     """Measure the coverage obtained by |trial_num| on |benchmark| using
     |fuzzer|."""
     initialize_logs()
@@ -636,8 +597,7 @@ def measure_trial_coverage(  # pylint: disable=invalid-name
         try:
             snapshot = measure_snapshot_coverage(measure_req.fuzzer,
                                                  measure_req.benchmark,
-                                                 measure_req.trial_id, cycle,
-                                                 region_coverage)
+                                                 measure_req.trial_id, cycle)
             if not snapshot:
                 break
             q.put(snapshot)
@@ -653,8 +613,8 @@ def measure_trial_coverage(  # pylint: disable=invalid-name
 
 
 def measure_snapshot_coverage(  # pylint: disable=too-many-locals
-        fuzzer: str, benchmark: str, trial_num: int, cycle: int,
-        region_coverage: bool) -> models.Snapshot:
+        fuzzer: str, benchmark: str, trial_num: int,
+        cycle: int) -> models.Snapshot:
     """Measure coverage of the snapshot for |cycle| for |trial_num| of |fuzzer|
     and |benchmark|."""
     snapshot_logger = logs.Logger('measurer',
@@ -665,18 +625,18 @@ def measure_snapshot_coverage(  # pylint: disable=too-many-locals
                                       'cycle': str(cycle),
                                   })
     snapshot_measurer = SnapshotMeasurer(fuzzer, benchmark, trial_num,
-                                         snapshot_logger, region_coverage)
+                                         snapshot_logger)
 
     measuring_start_time = time.time()
     snapshot_logger.info('Measuring cycle: %d.', cycle)
     this_time = experiment_utils.get_cycle_time(cycle)
     if snapshot_measurer.is_cycle_unchanged(cycle):
         snapshot_logger.info('Cycle: %d is unchanged.', cycle)
-        branches_covered = snapshot_measurer.get_current_coverage()
+        regions_covered = snapshot_measurer.get_current_coverage()
         fuzzer_stats_data = snapshot_measurer.get_fuzzer_stats(cycle)
         return models.Snapshot(time=this_time,
                                trial_id=trial_num,
-                               edges_covered=branches_covered,
+                               edges_covered=regions_covered,
                                fuzzer_stats=fuzzer_stats_data,
                                crashes=[])
 
@@ -710,11 +670,11 @@ def measure_snapshot_coverage(  # pylint: disable=too-many-locals
     crashes = snapshot_measurer.process_crashes(cycle)
 
     # Get the coverage of the new corpus units.
-    branches_covered = snapshot_measurer.get_current_coverage()
+    regions_covered = snapshot_measurer.get_current_coverage()
     fuzzer_stats_data = snapshot_measurer.get_fuzzer_stats(cycle)
     snapshot = models.Snapshot(time=this_time,
                                trial_id=trial_num,
-                               edges_covered=branches_covered,
+                               edges_covered=regions_covered,
                                fuzzer_stats=fuzzer_stats_data,
                                crashes=crashes)
 
diff --git a/experiment/measurer/test_coverage_utils.py b/experiment/measurer/test_coverage_utils.py
index 91db86a..e2c42ab 100644
--- a/experiment/measurer/test_coverage_utils.py
+++ b/experiment/measurer/test_coverage_utils.py
@@ -24,12 +24,11 @@ def get_test_data_path(*subpaths):
     return os.path.join(TEST_DATA_PATH, *subpaths)
 
 
-def test_extract_covered_branches_from_summary_json(fs):
-    """Tests that extract_covered_branches_from_summary_json returns the covered
-    branches from summary json file."""
+def test_extract_covered_regions_from_summary_json(fs):
+    """Tests that extract_covered_regions_from_summary_json returns the covered
+    regions from summary json file."""
     summary_json_file = get_test_data_path('cov_summary.json')
     fs.add_real_file(summary_json_file, read_only=False)
-    covered_branches = coverage_utils. \
-    extract_covered_branches_from_summary_json(
+    covered_regions = coverage_utils.extract_covered_regions_from_summary_json(
         summary_json_file)
-    assert len(covered_branches) == 9
+    assert len(covered_regions) == 15
diff --git a/experiment/measurer/test_data/cov_summary.json b/experiment/measurer/test_data/cov_summary.json
index b31b619..2350239 100644
--- a/experiment/measurer/test_data/cov_summary.json
+++ b/experiment/measurer/test_data/cov_summary.json
@@ -1 +1 @@
-{"version":"2.0.0","type":"llvm.coverage.json.export","data":[{"files":[{"filename":"/home/test/fuzz_no_fuzzer.cc","segments":[[1,16,20,1,1],[1,17,20,1,1],[1,20,20,1,0],[1,24,2,1,1],[1,27,20,1,0],[1,28,0,0,0],[2,37,2,1,1],[3,24,22,1,1],[3,30,2,1,0],[3,32,20,1,1],[3,35,2,1,0],[3,36,20,1,0],[3,37,20,1,1],[3,39,20,1,1],[3,42,20,1,0],[3,48,2,1,0],[5,3,0,1,1],[6,2,0,0,0],[7,12,1,1,1],[11,3,0,1,1],[12,2,0,0,0]],"expansions":[{"source_region":[3,39,3,42,10,0,1,1],"target_regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]},{"source_region":[3,39,3,42,10,0,1,1],"target_regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]}],"summary":{"lines":{"count":11,"covered":9,"percent":81},"functions":{"count":2,"covered":2,"percent":100},"instantiations":{"count":3,"covered":3,"percent":100},"regions":{"count":10,"covered":8,"notcovered":2,"percent":80}}}],"functions":[{"branches":[[626,5,626,12,1,0,0,0,4],[626,16,626,24,0,0,0,0,4]],"name":"main","count":1,"regions":[[7,12,12,2,1,0,0,0],[11,3,12,2,0,0,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc"]},{"branches":[[1306,5,1306,31,0,1,0,0,4],[1309,52,1309,64,0,1,0,0,4],[1327,3,1327,21,0,1,0,0,4],[1328,3,1328,15,0,1,0,0,4],[1329,3,1329,17,1,0,0,0,4],[1341,5,1341,13,0,1,0,0,4]],"name":"_Z3fooIiEvT_","count":1,"regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]},{"branches":[[341,5,341,29,0,1,0,0,4],[341,33,341,71,0,0,0,0,4],[344,11,344,22,3,0,0,0,4]],"name":"_Z3fooIfEvT_","count":1,"regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]}],"totals":{"lines":{"count":11,"covered":9,"percent":81},"functions":{"count":2,"covered":2,"percent":100},"instantiations":{"count":3,"covered":3,"percent":100},"regions":{"count":10,"covered":8,"notcovered":2,"percent":80},"branches":{"count":10,"covered":7,"notcovered":3,"percent":70}}}]}
\ No newline at end of file
+{"version":"2.0.0","type":"llvm.coverage.json.export","data":[{"files":[{"filename":"/home/test/fuzz_no_fuzzer.cc","segments":[[1,16,20,1,1],[1,17,20,1,1],[1,20,20,1,0],[1,24,2,1,1],[1,27,20,1,0],[1,28,0,0,0],[2,37,2,1,1],[3,24,22,1,1],[3,30,2,1,0],[3,32,20,1,1],[3,35,2,1,0],[3,36,20,1,0],[3,37,20,1,1],[3,39,20,1,1],[3,42,20,1,0],[3,48,2,1,0],[5,3,0,1,1],[6,2,0,0,0],[7,12,1,1,1],[11,3,0,1,1],[12,2,0,0,0]],"expansions":[{"source_region":[3,39,3,42,10,0,1,1],"target_regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]},{"source_region":[3,39,3,42,10,0,1,1],"target_regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]}],"summary":{"lines":{"count":11,"covered":9,"percent":81},"functions":{"count":2,"covered":2,"percent":100},"instantiations":{"count":3,"covered":3,"percent":100},"regions":{"count":10,"covered":8,"notcovered":2,"percent":80}}}],"functions":[{"name":"main","count":1,"regions":[[7,12,12,2,1,0,0,0],[11,3,12,2,0,0,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc"]},{"name":"_Z3fooIiEvT_","count":1,"regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]},{"name":"_Z3fooIfEvT_","count":1,"regions":[[2,37,6,2,1,0,0,0],[3,24,3,30,11,0,0,0],[3,32,3,35,10,0,0,0],[3,36,3,37,10,0,0,3],[3,37,3,48,10,0,0,0],[3,39,3,42,10,0,1,1],[5,3,6,2,0,0,0,0],[1,16,1,28,10,1,0,0],[1,17,1,20,10,1,0,0],[1,24,1,27,1,1,0,0]],"filenames":["/home/test/fuzz_no_fuzzer.cc","/home/test/fuzz_no_fuzzer.cc"]}],"totals":{"lines":{"count":11,"covered":9,"percent":81},"functions":{"count":2,"covered":2,"percent":100},"instantiations":{"count":3,"covered":3,"percent":100},"regions":{"count":10,"covered":8,"notcovered":2,"percent":80}}}]}
\ No newline at end of file
diff --git a/experiment/measurer/test_data/llvm_tools/llvm-cov b/experiment/measurer/test_data/llvm_tools/llvm-cov
index 35582db..858c581 100755
Binary files a/experiment/measurer/test_data/llvm_tools/llvm-cov and b/experiment/measurer/test_data/llvm_tools/llvm-cov differ
diff --git a/experiment/measurer/test_data/llvm_tools/llvm-profdata b/experiment/measurer/test_data/llvm_tools/llvm-profdata
index 668cb54..0670470 100755
Binary files a/experiment/measurer/test_data/llvm_tools/llvm-profdata and b/experiment/measurer/test_data/llvm_tools/llvm-profdata differ
diff --git a/experiment/measurer/test_data/test_measure_snapshot_coverage/freetype2-2017-coverage b/experiment/measurer/test_data/test_measure_snapshot_coverage/freetype2-2017-coverage
index 5b94983..13a41e7 100755
Binary files a/experiment/measurer/test_data/test_measure_snapshot_coverage/freetype2-2017-coverage and b/experiment/measurer/test_data/test_measure_snapshot_coverage/freetype2-2017-coverage differ
diff --git a/experiment/measurer/test_measure_manager.py b/experiment/measurer/test_measure_manager.py
index 29be769..f52bd3d 100644
--- a/experiment/measurer/test_measure_manager.py
+++ b/experiment/measurer/test_measure_manager.py
@@ -42,7 +42,6 @@ GIT_HASH = 'FAKE-GIT-HASH'
 CYCLE = 1
 
 SNAPSHOT_LOGGER = measure_manager.logger
-REGION_COVERAGE = False
 
 # pylint: disable=unused-argument,invalid-name,redefined-outer-name,protected-access
 
@@ -60,34 +59,34 @@ def db_experiment(experiment_config, db):
 def test_get_current_coverage(fs, experiment):
     """Tests that get_current_coverage reads the correct data from json file."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     json_cov_summary_file = get_test_data_path('cov_summary.json')
     fs.add_real_file(json_cov_summary_file, read_only=False)
     snapshot_measurer.cov_summary_file = json_cov_summary_file
-    covered_branches = snapshot_measurer.get_current_coverage()
-    assert covered_branches == 7
+    covered_regions = snapshot_measurer.get_current_coverage()
+    assert covered_regions == 8
 
 
 def test_get_current_coverage_error(fs, experiment):
     """Tests that get_current_coverage returns None from a
     defective json file."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     json_cov_summary_file = get_test_data_path('cov_summary_defective.json')
     fs.add_real_file(json_cov_summary_file, read_only=False)
     snapshot_measurer.cov_summary_file = json_cov_summary_file
-    covered_branches = snapshot_measurer.get_current_coverage()
-    assert not covered_branches
+    covered_regions = snapshot_measurer.get_current_coverage()
+    assert not covered_regions
 
 
 def test_get_current_coverage_no_file(fs, experiment):
     """Tests that get_current_coverage returns None with no json file."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     json_cov_summary_file = get_test_data_path('cov_summary_not_exist.json')
     snapshot_measurer.cov_summary_file = json_cov_summary_file
-    covered_branches = snapshot_measurer.get_current_coverage()
-    assert not covered_branches
+    covered_regions = snapshot_measurer.get_current_coverage()
+    assert not covered_regions
 
 
 @mock.patch('common.new_process.execute')
@@ -95,7 +94,7 @@ def test_generate_profdata_create(mocked_execute, experiment, fs):
     """Tests that generate_profdata can run the correct command."""
     mocked_execute.return_value = new_process.ProcessResult(0, '', False)
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     snapshot_measurer.profdata_file = '/work/reports/data.profdata'
     snapshot_measurer.profraw_file_pattern = '/work/reports/data-%m.profraw'
     profraw_file = '/work/reports/data-123.profraw'
@@ -117,7 +116,7 @@ def test_generate_profdata_merge(mocked_execute, experiment, fs):
     """Tests that generate_profdata can run correctly with existing profraw."""
     mocked_execute.return_value = new_process.ProcessResult(0, '', False)
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     snapshot_measurer.profdata_file = '/work/reports/data.profdata'
     snapshot_measurer.profraw_file_pattern = '/work/reports/data-%m.profraw'
     profraw_file = '/work/reports/data-123.profraw'
@@ -145,7 +144,7 @@ def test_generate_summary(mocked_get_coverage_binary, mocked_execute,
     mocked_get_coverage_binary.return_value = coverage_binary_path
 
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     snapshot_measurer.cov_summary_file = "/reports/cov_summary.txt"
     snapshot_measurer.profdata_file = "/reports/data.profdata"
     fs.create_dir('/reports')
@@ -177,9 +176,9 @@ def test_measure_trial_coverage(mocked_measure_snapshot_coverage, mocked_queue,
     measure_request = measure_manager.SnapshotMeasureRequest(
         FUZZER, BENCHMARK, TRIAL_NUM, min_cycle)
     measure_manager.measure_trial_coverage(measure_request, max_cycle,
-                                           mocked_queue(), False)
+                                           mocked_queue())
     expected_calls = [
-        mock.call(FUZZER, BENCHMARK, TRIAL_NUM, cycle, False)
+        mock.call(FUZZER, BENCHMARK, TRIAL_NUM, cycle)
         for cycle in range(min_cycle, max_cycle + 1)
     ]
     assert mocked_measure_snapshot_coverage.call_args_list == expected_calls
@@ -192,7 +191,7 @@ def test_measure_all_trials_not_ready(mocked_rsync, mocked_ls, experiment):
     mocked_ls.return_value = new_process.ProcessResult(1, '', False)
     assert measure_manager.measure_all_trials(
         experiment_utils.get_experiment_name(), MAX_TOTAL_TIME,
-        test_utils.MockPool(), queue.Queue(), False)
+        test_utils.MockPool(), queue.Queue())
     assert not mocked_rsync.called
 
 
@@ -209,14 +208,14 @@ def test_measure_all_trials_no_more(mocked_directories_have_same_files,
     mock_pool = test_utils.MockPool()
     assert not measure_manager.measure_all_trials(
         experiment_utils.get_experiment_name(), MAX_TOTAL_TIME, mock_pool,
-        queue.Queue(), False)
+        queue.Queue())
 
 
 def test_is_cycle_unchanged_doesnt_exist(experiment):
     """Test that is_cycle_unchanged can properly determine if a cycle is
     unchanged or not when it needs to copy the file for the first time."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     this_cycle = 1
     with test_utils.mock_popen_ctx_mgr(returncode=1):
         assert not snapshot_measurer.is_cycle_unchanged(this_cycle)
@@ -228,7 +227,7 @@ def test_is_cycle_unchanged_first_copy(mocked_read, mocked_cp, experiment):
     """Test that is_cycle_unchanged can properly determine if a cycle is
     unchanged or not when it needs to copy the file for the first time."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     this_cycle = 100
     unchanged_cycles_file_contents = (
         '\n'.join([str(num) for num in range(10)] + [str(this_cycle)]))
@@ -243,7 +242,7 @@ def test_is_cycle_unchanged_update(fs, experiment):
     """Test that is_cycle_unchanged can properly determine that a
     cycle has changed when it has the file but needs to update it."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
 
     this_cycle = 100
     initial_unchanged_cycles_file_contents = (
@@ -267,7 +266,7 @@ def test_is_cycle_unchanged_skip_cp(mocked_cp, fs, experiment):
     """Check that is_cycle_unchanged doesn't call filestore_utils.cp
     unnecessarily."""
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     this_cycle = 100
     initial_unchanged_cycles_file_contents = (
         '\n'.join([str(num) for num in range(10)] + [str(this_cycle + 1)]))
@@ -283,7 +282,7 @@ def test_is_cycle_unchanged_no_file(mocked_cp, fs, experiment):
     unchanged-cycles file."""
     # Make sure we log if there is no unchanged-cycles file.
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     mocked_cp.return_value = new_process.ProcessResult(1, '', False)
     assert not snapshot_measurer.is_cycle_unchanged(0)
 
@@ -300,7 +299,7 @@ def test_run_cov_new_units(_, mocked_execute, fs, environ):
     }
     mocked_execute.return_value = new_process.ProcessResult(0, '', False)
     snapshot_measurer = measure_manager.SnapshotMeasurer(
-        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER, REGION_COVERAGE)
+        FUZZER, BENCHMARK, TRIAL_NUM, SNAPSHOT_LOGGER)
     snapshot_measurer.initialize_measurement_dirs()
     shared_units = ['shared1', 'shared2']
     fs.create_file(snapshot_measurer.measured_files_path,
@@ -378,7 +377,7 @@ class TestIntegrationMeasurement:
         # fakefs. A directory containing necessary llvm tools is also added to
         # PATH.
         llvm_tools_path = get_test_data_path('llvm_tools')
-        os.environ['PATH'] += os.pathsep + llvm_tools_path
+        os.environ["PATH"] += os.pathsep + llvm_tools_path
         os.environ['WORK'] = str(tmp_path)
         mocked_is_cycle_unchanged.return_value = False
         # Set up the coverage binary.
@@ -403,8 +402,7 @@ class TestIntegrationMeasurement:
         db_utils.add_all([trial])
 
         snapshot_measurer = measure_manager.SnapshotMeasurer(
-            trial.fuzzer, trial.benchmark, trial.id, SNAPSHOT_LOGGER,
-            REGION_COVERAGE)
+            trial.fuzzer, trial.benchmark, trial.id, SNAPSHOT_LOGGER)
 
         # Set up the snapshot archive.
         cycle = 1
@@ -420,10 +418,10 @@ class TestIntegrationMeasurement:
             # integration tests.
             snapshot = measure_manager.measure_snapshot_coverage(
                 snapshot_measurer.fuzzer, snapshot_measurer.benchmark,
-                snapshot_measurer.trial_num, cycle, False)
+                snapshot_measurer.trial_num, cycle)
         assert snapshot
         assert snapshot.time == cycle * experiment_utils.get_snapshot_seconds()
-        assert snapshot.edges_covered == 4629
+        assert snapshot.edges_covered == 13178
 
 
 @pytest.mark.parametrize('archive_name',
diff --git a/experiment/resources/runner-startup-script-template.sh b/experiment/resources/runner-startup-script-template.sh
index 31b1bfe..00945fd 100644
--- a/experiment/resources/runner-startup-script-template.sh
+++ b/experiment/resources/runner-startup-script-template.sh
@@ -36,18 +36,15 @@ done{% endif %}
 
 docker run \
 --privileged --cpus={{num_cpu_cores}} --rm \
-{% if cpuset %}--cpuset-cpus={{cpuset}} {% endif %}\
 -e INSTANCE_NAME={{instance_name}} \
 -e FUZZER={{fuzzer}} \
 -e BENCHMARK={{benchmark}} \
 -e EXPERIMENT={{experiment}} \
 -e TRIAL_ID={{trial_id}} \
 -e MAX_TOTAL_TIME={{max_total_time}} \
--e SNAPSHOT_PERIOD={{snapshot_period}} \
 -e NO_SEEDS={{no_seeds}} \
 -e NO_DICTIONARIES={{no_dictionaries}} \
 -e OSS_FUZZ_CORPUS={{oss_fuzz_corpus}} \
--e CUSTOM_SEED_CORPUS_DIR={{custom_seed_corpus_dir}} \
 -e DOCKER_REGISTRY={{docker_registry}} {% if not local_experiment %}-e CLOUD_PROJECT={{cloud_project}} -e CLOUD_COMPUTE_ZONE={{cloud_compute_zone}} {% endif %}\
 -e EXPERIMENT_FILESTORE={{experiment_filestore}} {% if local_experiment %}-v {{experiment_filestore}}:{{experiment_filestore}} {% endif %}\
 -e REPORT_FILESTORE={{report_filestore}} {% if local_experiment %}-v {{report_filestore}}:{{report_filestore}} {% endif %}\
@@ -55,5 +52,4 @@ docker run \
 -e LOCAL_EXPERIMENT={{local_experiment}} \
 {% if not local_experiment %}--name=runner-container {% endif %}\
 --cap-add SYS_NICE --cap-add SYS_PTRACE \
---security-opt seccomp=unconfined \
 {{docker_image_url}} 2>&1 | tee /tmp/runner-log.txt
diff --git a/experiment/run_experiment.py b/experiment/run_experiment.py
index cfb995b..61bb675 100644
--- a/experiment/run_experiment.py
+++ b/experiment/run_experiment.py
@@ -77,8 +77,6 @@ def read_and_validate_experiment_config(config_filename: str) -> Dict:
     bool_params = {'private', 'merge_with_nonprivate'}
 
     local_experiment = config.get('local_experiment', False)
-    snapshot_period = config.get('snapshot_period',
-                                 experiment_utils.DEFAULT_SNAPSHOT_SECONDS)
     if not local_experiment:
         required_params = required_params.union(cloud_config)
 
@@ -135,7 +133,6 @@ def read_and_validate_experiment_config(config_filename: str) -> Dict:
         raise ValidationError('Config: %s is invalid.' % config_filename)
 
     config['local_experiment'] = local_experiment
-    config['snapshot_period'] = snapshot_period
     return config
 
 
@@ -151,26 +148,6 @@ def get_directories(parent_dir):
     ]
 
 
-# pylint: disable=too-many-locals
-def validate_custom_seed_corpus(custom_seed_corpus_dir, benchmarks):
-    """Validate seed corpus provided by user"""
-    if not os.path.isdir(custom_seed_corpus_dir):
-        raise ValidationError('Corpus location "%s" is invalid.' %
-                              custom_seed_corpus_dir)
-
-    for benchmark in benchmarks:
-        benchmark_corpus_dir = os.path.join(custom_seed_corpus_dir, benchmark)
-        if not os.path.exists(benchmark_corpus_dir):
-            raise ValidationError('Custom seed corpus directory for '
-                                  'benchmark "%s" does not exist.' % benchmark)
-        if not os.path.isdir(benchmark_corpus_dir):
-            raise ValidationError('Seed corpus of benchmark "%s" must be '
-                                  'a directory.' % benchmark)
-        if not os.listdir(benchmark_corpus_dir):
-            raise ValidationError('Seed corpus of benchmark "%s" is empty.' %
-                                  benchmark)
-
-
 def validate_benchmarks(benchmarks: List[str]):
     """Parses and validates list of benchmarks."""
     benchmark_types = set()
@@ -240,11 +217,7 @@ def start_experiment(  # pylint: disable=too-many-arguments
         no_dictionaries=False,
         oss_fuzz_corpus=False,
         allow_uncommitted_changes=False,
-        concurrent_builds=None,
-        measurers_cpus=None,
-        runners_cpus=None,
-        region_coverage=False,
-        custom_seed_corpus_dir=None):
+        concurrent_builds=None):
     """Start a fuzzer benchmarking experiment."""
     if not allow_uncommitted_changes:
         check_no_uncommitted_changes()
@@ -262,24 +235,13 @@ def start_experiment(  # pylint: disable=too-many-arguments
     config['oss_fuzz_corpus'] = oss_fuzz_corpus
     config['description'] = description
     config['concurrent_builds'] = concurrent_builds
-    config['measurers_cpus'] = measurers_cpus
-    config['runners_cpus'] = runners_cpus
     config['runner_machine_type'] = config.get('runner_machine_type',
                                                'n1-standard-1')
     config['runner_num_cpu_cores'] = config.get('runner_num_cpu_cores', 1)
-    assert (runners_cpus is None or
-            runners_cpus >= config['runner_num_cpu_cores'])
     # Note this is only used if runner_machine_type is None.
     # 12GB is just the amount that KLEE needs, use this default to make KLEE
     # experiments easier to run.
     config['runner_memory'] = config.get('runner_memory', '12GB')
-    config['region_coverage'] = region_coverage
-
-    config['custom_seed_corpus_dir'] = custom_seed_corpus_dir
-    if config['custom_seed_corpus_dir']:
-        validate_custom_seed_corpus(config['custom_seed_corpus_dir'],
-                                    benchmarks)
-
     return start_experiment_from_full_config(config)
 
 
@@ -362,16 +324,6 @@ def copy_resources_to_bucket(config_dir: str, config: Dict):
         for benchmark in config['benchmarks']:
             add_oss_fuzz_corpus(benchmark, oss_fuzz_corpora_dir)
 
-    if config['custom_seed_corpus_dir']:
-        for benchmark in config['benchmarks']:
-            benchmark_custom_corpus_dir = os.path.join(
-                config['custom_seed_corpus_dir'], benchmark)
-            filestore_utils.cp(
-                benchmark_custom_corpus_dir,
-                experiment_utils.get_custom_seed_corpora_filestore_path() + '/',
-                recursive=True,
-                parallel=True)
-
 
 class BaseDispatcher:
     """Class representing the dispatcher."""
@@ -421,8 +373,6 @@ class LocalDispatcher(BaseDispatcher):
         set_report_filestore_arg = (
             'REPORT_FILESTORE={report_filestore}'.format(
                 report_filestore=self.config['report_filestore']))
-        set_snapshot_period_arg = 'SNAPSHOT_PERIOD={snapshot_period}'.format(
-            snapshot_period=self.config['snapshot_period'])
         docker_image_url = '{docker_registry}/dispatcher-image'.format(
             docker_registry=docker_registry)
         command = [
@@ -445,8 +395,6 @@ class LocalDispatcher(BaseDispatcher):
             '-e',
             set_experiment_filestore_arg,
             '-e',
-            set_snapshot_period_arg,
-            '-e',
             set_report_filestore_arg,
             '-e',
             set_docker_registry_arg,
@@ -560,18 +508,6 @@ def main():
                         '--concurrent-builds',
                         help='Max concurrent builds allowed.',
                         required=False)
-    parser.add_argument('-mc',
-                        '--measurers-cpus',
-                        help='Cpus available to the measurers.',
-                        required=False)
-    parser.add_argument('-rc',
-                        '--runners-cpus',
-                        help='Cpus available to the runners.',
-                        required=False)
-    parser.add_argument('-cs',
-                        '--custom-seed-corpus-dir',
-                        help='Path to the custom seed corpus',
-                        required=False)
 
     all_fuzzers = fuzzer_utils.get_fuzzer_names()
     parser.add_argument('-f',
@@ -599,12 +535,6 @@ def main():
                         required=False,
                         default=False,
                         action='store_true')
-    parser.add_argument('-cr',
-                        '--region-coverage',
-                        help='Use region as coverage metric.',
-                        required=False,
-                        default=False,
-                        action='store_true')
     parser.add_argument(
         '-o',
         '--oss-fuzz-corpus',
@@ -619,35 +549,8 @@ def main():
     if concurrent_builds is not None:
         if not concurrent_builds.isdigit():
             parser.error(
-                'The concurrent build argument must be a positive number')
+                "The concurrent build argument must be a positive number")
         concurrent_builds = int(concurrent_builds)
-    runners_cpus = args.runners_cpus
-    if runners_cpus is not None:
-        if not runners_cpus.isdigit():
-            parser.error('The runners cpus argument must be a positive number')
-        runners_cpus = int(runners_cpus)
-    measurers_cpus = args.measurers_cpus
-    if measurers_cpus is not None:
-        if not measurers_cpus.isdigit():
-            parser.error(
-                'The measurers cpus argument must be a positive number')
-        if runners_cpus is None:
-            parser.error(
-                'With the measurers cpus argument you need to specify the'
-                ' runners cpus argument too')
-        measurers_cpus = int(measurers_cpus)
-    if (runners_cpus if runners_cpus else 0) + (measurers_cpus if measurers_cpus
-                                                else 0) > os.cpu_count():
-        parser.error('The sum of runners and measurers cpus is greater than the'
-                     ' available cpu cores (%d)' % os.cpu_count())
-
-    if args.custom_seed_corpus_dir:
-        if args.no_seeds:
-            parser.error('Cannot enable options "custom_seed_corpus_dir" and '
-                         '"no_seeds" at the same time')
-        if args.oss_fuzz_corpus:
-            parser.error('Cannot enable options "custom_seed_corpus_dir" and '
-                         '"oss_fuzz_corpus" at the same time')
 
     start_experiment(args.experiment_name,
                      args.experiment_config,
@@ -658,11 +561,7 @@ def main():
                      no_dictionaries=args.no_dictionaries,
                      oss_fuzz_corpus=args.oss_fuzz_corpus,
                      allow_uncommitted_changes=args.allow_uncommitted_changes,
-                     concurrent_builds=concurrent_builds,
-                     measurers_cpus=measurers_cpus,
-                     runners_cpus=runners_cpus,
-                     region_coverage=args.region_coverage,
-                     custom_seed_corpus_dir=args.custom_seed_corpus_dir)
+                     concurrent_builds=concurrent_builds)
     return 0
 
 
diff --git a/experiment/runner.py b/experiment/runner.py
index dd9329e..94cc930 100644
--- a/experiment/runner.py
+++ b/experiment/runner.py
@@ -115,17 +115,6 @@ def get_clusterfuzz_seed_corpus_path(fuzz_target_path):
     return seed_corpus_path if os.path.exists(seed_corpus_path) else None
 
 
-def _copy_custom_seed_corpus(corpus_directory):
-    "Copy custom seed corpus provided by user"
-    shutil.rmtree(corpus_directory)
-    benchmark = environment.get('BENCHMARK')
-    benchmark_custom_corpus_dir = posixpath.join(
-        experiment_utils.get_custom_seed_corpora_filestore_path(), benchmark)
-    filestore_utils.cp(benchmark_custom_corpus_dir,
-                       corpus_directory,
-                       recursive=True)
-
-
 def _unpack_clusterfuzz_seed_corpus(fuzz_target_path, corpus_directory):
     """If a clusterfuzz seed corpus archive is available, unpack it into the
     corpus directory if it exists. Copied from unpack_seed_corpus in
@@ -183,10 +172,7 @@ def run_fuzzer(max_total_time, log_filename):
         logs.error('Fuzz target binary not found.')
         return
 
-    if environment.get('CUSTOM_SEED_CORPUS_DIR'):
-        _copy_custom_seed_corpus(input_corpus)
-    else:
-        _unpack_clusterfuzz_seed_corpus(target_binary, input_corpus)
+    _unpack_clusterfuzz_seed_corpus(target_binary, input_corpus)
     _clean_seed_corpus(input_corpus)
 
     if max_total_time is None:
diff --git a/experiment/scheduler.py b/experiment/scheduler.py
index bf9af20..9f4d6f4 100644
--- a/experiment/scheduler.py
+++ b/experiment/scheduler.py
@@ -116,30 +116,22 @@ def delete_instances(instances, experiment_config):
                                    experiment_config['cloud_compute_zone'])
 
 
-def end_expired_trials(experiment_config: dict, core_allocation: dict):
+def end_expired_trials(experiment_config: dict):
     """Get all expired trials, end them and return them."""
     trials_past_expiry = get_expired_trials(experiment_config['experiment'],
                                             experiment_config['max_total_time'])
     expired_instances = []
-    expired_trial_ids = []
     current_dt = datetime_now()
     for trial in trials_past_expiry:
-        trial_id = trial.id
         expired_instances.append(
             experiment_utils.get_trial_instance_name(
-                experiment_config['experiment'], trial_id))
-        expired_trial_ids.append(trial_id)
+                experiment_config['experiment'], trial.id))
         trial.time_ended = current_dt
 
     # Bail out here because trials_past_expiry will be truthy until evaluated.
     if not expired_instances:
         return
 
-    if core_allocation is not None:
-        for cpuset, trial_id in core_allocation.items():
-            if trial_id in expired_trial_ids:
-                core_allocation[cpuset] = None
-
     if not experiment_utils.is_local_experiment() and not delete_instances(
             expired_instances, experiment_config):
         # If we failed to delete some instances, then don't update the status
@@ -522,18 +514,17 @@ def replace_trial(trial, preemptible):
     return replacement
 
 
-def schedule(experiment_config: dict, pool, core_allocation=None):
+def schedule(experiment_config: dict, pool):
     """Gets all pending trials for the current experiment and then schedules
     those that are possible."""
     logger.info('Finding trials to schedule.')
 
     # End expired trials
-    end_expired_trials(experiment_config, core_allocation)
+    end_expired_trials(experiment_config)
 
     # Start pending trials.
     pending_trials = list(get_pending_trials(experiment_config['experiment']))
-    started_trials = start_trials(pending_trials, experiment_config, pool,
-                                  core_allocation)
+    started_trials = start_trials(pending_trials, experiment_config, pool)
     return started_trials
 
 
@@ -548,31 +539,12 @@ def schedule_loop(experiment_config: dict):
     num_trials = len(
         get_experiment_trials(experiment_config['experiment']).all())
     local_experiment = experiment_utils.is_local_experiment()
-    pool_args = ()
-    core_allocation = None
-    runners_cpus = experiment_config['runners_cpus']
-    if runners_cpus is not None:
-        if local_experiment:
-            runner_num_cpu_cores = experiment_config['runner_num_cpu_cores']
-            processes = runners_cpus // runner_num_cpu_cores
-            logger.info('Scheduling runners from core 0 to %d.' %
-                        (runner_num_cpu_cores * processes - 1))
-            core_allocation = {}
-            for cpu in range(0, runner_num_cpu_cores * processes,
-                             runner_num_cpu_cores):
-                core_allocation['%d-%d' %
-                                (cpu, cpu + runner_num_cpu_cores - 1)] = None
-            pool_args = (processes,)
-        else:
-            pool_args = (runners_cpus,)
-
     if not local_experiment:
         gce.initialize()
         trial_instance_manager = TrialInstanceManager(num_trials,
                                                       experiment_config)
-
     experiment = experiment_config['experiment']
-    with multiprocessing.Pool(*pool_args) as pool:
+    with multiprocessing.Pool() as pool:
         handle_preempted = False
         while not all_trials_ended(experiment):
             try:
@@ -585,7 +557,7 @@ def schedule_loop(experiment_config: dict):
                     #    initial trial was started.
                     handle_preempted = True
 
-                schedule(experiment_config, pool, core_allocation)
+                schedule(experiment_config, pool)
                 if handle_preempted:
                     trial_instance_manager.handle_preempted_trials()
             except Exception:  # pylint: disable=broad-except
@@ -601,7 +573,7 @@ def schedule_loop(experiment_config: dict):
     logger.info('Finished scheduling.')
 
 
-def update_started_trials(trial_proxies, trial_id_mapping, core_allocation):
+def update_started_trials(trial_proxies, trial_id_mapping):
     """Update started trials in |trial_id_mapping| with results from
     |trial_proxies| and save the updated trials."""
     # Map proxies back to trials and mark trials as started when proxies were
@@ -612,17 +584,13 @@ def update_started_trials(trial_proxies, trial_id_mapping, core_allocation):
             continue
         trial = trial_id_mapping[proxy.id]
         trial.time_started = proxy.time_started
-
-        if core_allocation is not None:
-            core_allocation[proxy.cpuset] = proxy.id
-
         started_trials.append(trial)
     if started_trials:
         db_utils.add_all(started_trials)
     return started_trials
 
 
-def start_trials(trials, experiment_config: dict, pool, core_allocation=None):
+def start_trials(trials, experiment_config: dict, pool):
     """Start all |trials| that are possible to start. Marks the ones that were
     started as started."""
     logger.info('Starting trials.')
@@ -637,25 +605,13 @@ def start_trials(trials, experiment_config: dict, pool, core_allocation=None):
     shuffled_trials = list(trial_id_mapping.values())
     random.shuffle(shuffled_trials)
 
-    free_cpusets = [
-        cpuset for cpuset, trial_id in core_allocation.items()
-        if trial_id is None
-    ] if core_allocation is not None else None
-
-    start_trial_args = []
-    for index, trial in enumerate(shuffled_trials):
-        if free_cpusets is not None and index >= len(free_cpusets):
-            break
-
-        start_trial_args += [
-            (TrialProxy(trial), experiment_config,
-             free_cpusets[index] if free_cpusets is not None else None)
-        ]
-
+    start_trial_args = [
+        (TrialProxy(trial), experiment_config) for trial in shuffled_trials
+    ]
     started_trial_proxies = pool.starmap(_start_trial, start_trial_args)
     started_trials = update_started_trials(started_trial_proxies,
-                                           trial_id_mapping, core_allocation)
-    logger.info(f'Started {len(started_trials)} trials.')
+                                           trial_id_mapping)
+    logger.info('Done starting trials.')
     return started_trials
 
 
@@ -670,7 +626,6 @@ class TrialProxy:
         self.time_started = trial.time_started
         self.time_ended = trial.time_ended
         self.preemptible = trial.preemptible
-        self.cpuset = None
 
 
 def _initialize_logs(experiment):
@@ -687,7 +642,7 @@ def _initialize_logs(experiment):
 # https://cloud.google.com/compute/docs/instances/preemptible#preemption_selection
 
 
-def _start_trial(trial: TrialProxy, experiment_config: dict, cpuset=None):
+def _start_trial(trial: TrialProxy, experiment_config: dict):
     """Start a trial if possible. Mark the trial as started if it was and then
     return the Trial. Otherwise return None."""
     # TODO(metzman): Add support for early exit (trial_creation_failed) that was
@@ -698,23 +653,17 @@ def _start_trial(trial: TrialProxy, experiment_config: dict, cpuset=None):
     _initialize_logs(experiment_config['experiment'])
     logger.info('Start trial %d.', trial.id)
     started = create_trial_instance(trial.fuzzer, trial.benchmark, trial.id,
-                                    experiment_config, trial.preemptible,
-                                    cpuset)
+                                    experiment_config, trial.preemptible)
     if started:
         trial.time_started = datetime_now()
-        trial.cpuset = cpuset
         return trial
     logger.info('Trial: %d not started.', trial.id)
     return None
 
 
-def render_startup_script_template(  # pylint: disable=too-many-arguments
-        instance_name: str,
-        fuzzer: str,
-        benchmark: str,
-        trial_id: int,
-        experiment_config: dict,
-        cpuset=None):
+def render_startup_script_template(instance_name: str, fuzzer: str,
+                                   benchmark: str, trial_id: int,
+                                   experiment_config: dict):
     """Render the startup script using the template and the parameters
     provided and return the result."""
     experiment = experiment_config['experiment']
@@ -731,7 +680,6 @@ def render_startup_script_template(  # pylint: disable=too-many-arguments
         'fuzzer': fuzzer,
         'trial_id': trial_id,
         'max_total_time': experiment_config['max_total_time'],
-        'snapshot_period': experiment_config['snapshot_period'],
         'experiment_filestore': experiment_config['experiment_filestore'],
         'report_filestore': experiment_config['report_filestore'],
         'fuzz_target': fuzz_target,
@@ -742,8 +690,6 @@ def render_startup_script_template(  # pylint: disable=too-many-arguments
         'no_dictionaries': experiment_config['no_dictionaries'],
         'oss_fuzz_corpus': experiment_config['oss_fuzz_corpus'],
         'num_cpu_cores': experiment_config['runner_num_cpu_cores'],
-        'cpuset': cpuset,
-        'custom_seed_corpus_dir': experiment_config['custom_seed_corpus_dir'],
     }
 
     if not local_experiment:
@@ -753,20 +699,15 @@ def render_startup_script_template(  # pylint: disable=too-many-arguments
     return template.render(**kwargs)
 
 
-def create_trial_instance(  # pylint: disable=too-many-arguments
-        fuzzer: str,
-        benchmark: str,
-        trial_id: int,
-        experiment_config: dict,
-        preemptible: bool,
-        cpuset=None) -> bool:
+def create_trial_instance(fuzzer: str, benchmark: str, trial_id: int,
+                          experiment_config: dict, preemptible: bool) -> bool:
     """Create or start a trial instance for a specific
     trial_id,fuzzer,benchmark."""
     instance_name = experiment_utils.get_trial_instance_name(
         experiment_config['experiment'], trial_id)
     startup_script = render_startup_script_template(instance_name, fuzzer,
                                                     benchmark, trial_id,
-                                                    experiment_config, cpuset)
+                                                    experiment_config)
     startup_script_path = '/tmp/%s-start-docker.sh' % instance_name
     with open(startup_script_path, 'w') as file_handle:
         file_handle.write(startup_script)
diff --git a/experiment/stop_experiment.py b/experiment/stop_experiment.py
index 44aed99..6b648c3 100644
--- a/experiment/stop_experiment.py
+++ b/experiment/stop_experiment.py
@@ -32,7 +32,6 @@ def stop_experiment(experiment_name, experiment_config_filename):
         raise NotImplementedError(
             'Local experiment stop logic is not implemented.')
 
-    logger.info('Stopping experiment.')
     cloud_project = experiment_config['cloud_project']
     cloud_compute_zone = experiment_config['cloud_compute_zone']
 
@@ -55,7 +54,6 @@ def stop_experiment(experiment_name, experiment_config_filename):
         logger.warning('No experiment instances found, no work to do.')
         return True
 
-    logger.info('Deleting instance.')
     if not gcloud.delete_instances(experiment_instances, cloud_compute_zone):
         logger.error('Failed to stop experiment instances.')
         return False
diff --git a/experiment/test_data/experiment-config.yaml b/experiment/test_data/experiment-config.yaml
index 5c58d07..a988e4c 100644
--- a/experiment/test_data/experiment-config.yaml
+++ b/experiment/test_data/experiment-config.yaml
@@ -15,7 +15,6 @@
 experiment: test-experiment
 trials: 4
 max_total_time: 86400
-snapshot_period: 900
 cloud_project: fuzzbench
 docker_registry: gcr.io/fuzzbench
 cloud_compute_zone: us-central1-a
@@ -32,10 +31,7 @@ git_hash: "git-hash"
 no_seeds: false
 no_dictionaries: false
 oss_fuzz_corpus: false
-custom_seed_corpus_dir: null
 description: "Test experiment"
 concurrent_builds: null
-runners_cpus: null
-measurers_cpus: null
 runner_num_cpu_cores: 1
 runner_machine_type: 'n1-standard-1'
diff --git a/experiment/test_run_experiment.py b/experiment/test_run_experiment.py
index b12a332..47034fb 100644
--- a/experiment/test_run_experiment.py
+++ b/experiment/test_run_experiment.py
@@ -202,7 +202,6 @@ def test_copy_resources_to_bucket(tmp_path):
         'experiment': 'experiment',
         'benchmarks': ['libxslt_xpath'],
         'oss_fuzz_corpus': True,
-        'custom_seed_corpus_dir': None,
     }
     try:
         with mock.patch('common.filestore_utils.cp') as mocked_filestore_cp:
diff --git a/experiment/test_scheduler.py b/experiment/test_scheduler.py
index 61034e4..37186ff 100644
--- a/experiment/test_scheduler.py
+++ b/experiment/test_scheduler.py
@@ -108,18 +108,15 @@ done
 
 docker run \\
 --privileged --cpus=1 --rm \\
-\\
 -e INSTANCE_NAME=r-test-experiment-9 \\
 -e FUZZER=fuzzer-a \\
 -e BENCHMARK={benchmark} \\
 -e EXPERIMENT=test-experiment \\
 -e TRIAL_ID=9 \\
 -e MAX_TOTAL_TIME=86400 \\
--e SNAPSHOT_PERIOD=900 \\
 -e NO_SEEDS=False \\
 -e NO_DICTIONARIES=False \\
 -e OSS_FUZZ_CORPUS=False \\
--e CUSTOM_SEED_CORPUS_DIR=None \\
 -e DOCKER_REGISTRY=gcr.io/fuzzbench -e CLOUD_PROJECT=fuzzbench -e CLOUD_COMPUTE_ZONE=us-central1-a \\
 -e EXPERIMENT_FILESTORE=gs://experiment-data \\
 -e REPORT_FILESTORE=gs://web-reports \\
@@ -127,7 +124,6 @@ docker run \\
 -e LOCAL_EXPERIMENT=False \\
 --name=runner-container \\
 --cap-add SYS_NICE --cap-add SYS_PTRACE \\
---security-opt seccomp=unconfined \\
 {docker_image_url} 2>&1 | tee /tmp/runner-log.txt'''
     with mock.patch('common.benchmark_utils.get_fuzz_target',
                     return_value=expected_target):
@@ -157,7 +153,6 @@ def test_create_trial_instance_local_experiment(benchmark, expected_image,
 
 docker run \\
 --privileged --cpus=1 --rm \\
-\\
 -e INSTANCE_NAME=r-test-experiment-9 \\
 -e FUZZER=fuzzer-a \\
 -e BENCHMARK={benchmark} \\
@@ -171,7 +166,6 @@ docker run \\
 -e LOCAL_EXPERIMENT=True \\
 \\
 --cap-add SYS_NICE --cap-add SYS_PTRACE \\
---security-opt seccomp=unconfined \\
 {docker_image_url} 2>&1 | tee /tmp/runner-log.txt'''
     _test_create_trial_instance(benchmark, expected_image, expected_target,
                                 expected_startup_script,
 
diff --git a/requirements.txt b/requirements.txt
index a7c09d6..2722c2d 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -6,11 +6,9 @@ google-cloud-logging==1.15.1
 google-cloud-secret-manager==2.4.0
 clusterfuzz==0.0.1a0
 Jinja2==2.11.3
-numpy==1.21.0
-MarkupSafe==2.0.1
+numpy==1.18.1
 Orange3==3.28.0
 pandas==1.2.4
-psutil==5.9.0
 psycopg2-binary==2.8.4
 pyfakefs==3.7.1
 pytest==6.1.2
@@ -23,7 +21,6 @@ scikit-posthocs==0.6.2
 scipy==1.6.2
 seaborn==0.11.1
 sqlalchemy==1.3.19
-protobuf==3.20.1
 
 # Needed for development.
 pylint==2.7.4
diff --git a/service/automatic_run_experiment.py b/service/automatic_run_experiment.py
index 5459925..81a6058 100644
--- a/service/automatic_run_experiment.py
+++ b/service/automatic_run_experiment.py
@@ -212,26 +212,9 @@ def run_requested_experiment(dry_run):
 
     benchmark_type = requested_experiment.get('type')
     if benchmark_type == benchmark_utils.BenchmarkType.BUG.value:
-        valid_benchmarks = benchmark_utils.exclude_non_cpp(
-            benchmark_utils.get_bug_benchmarks())
+        benchmarks = benchmark_utils.get_bug_benchmarks()
     else:
-        valid_benchmarks = benchmark_utils.exclude_non_cpp(
-            benchmark_utils.get_coverage_benchmarks())
-
-    benchmarks = requested_experiment.get('benchmarks')
-    if benchmarks is None:
-        benchmarks = valid_benchmarks
-    else:
-        errors = False
-        for benchmark in benchmarks:
-            if benchmark not in valid_benchmarks:
-                logs.error(
-                    'Requested experiment:'
-                    ' in %s, %s is not a valid %s benchmark.',
-                    requested_experiment, benchmark, benchmark_type)
-                errors = True
-        if errors:
-            return
+        benchmarks = benchmark_utils.get_coverage_benchmarks()
 
     logs.info('Running experiment: %s with fuzzers: %s.', experiment_name,
               ' '.join(fuzzers))

diff --git a/third_party/oss-fuzz b/third_party/oss-fuzz
index d39cbb5..e25d795 160000
--- a/third_party/oss-fuzz
+++ b/third_party/oss-fuzz
@@ -1 +1 @@
-Subproject commit d39cbb5a8ce75f6187bd7731e0099dbd1c23a3b1
+Subproject commit e25d79502e175cb7bd1f9838b1f3971bc550b988
