# SeamFuzz-Artifact
This repository is for the implementation of our paper accepted in ICSE 2023, "Learning Seed-Adaptive Mutation Strategies for
Greybox Fuzzing".
Our tool, [SeamFuzz](https://github.com/kupl/SeamFuzz-public), is a grey-box mutation-based fuzzer built on [AFL++](https://github.com/AFLplusplus/AFLplusplus)-v3.15++.

# Requirements
Please refer to [REQUIREMENTS.md](./REQUIREMENTS.md) for the operating system and hardware requirements.

# Setup
Please refer to [INSTALL.md](./INSTALL.md) for installation.

# Structure
Please refer to [STRUCTURE.md](./STRUCTURE.md) for SeamFuzz structure description.

# Benchmarks
We offer all benchmarks used for our experiments and a script file for building each benchmarks in `benchmarks` directory.
Note that, we evaluated all benchmarks on FuzzBench framework.

You can follow the instructions written in `Dockerfile` and `build.sh` in each benchmark directory to 


# How to use
We provide an example instruction which conducts a short experiment running **AFL++**, **AFL++_MOpt**, and **SeamFuzz** on benchmark programs ... with trials during 5 hours. 
Note that conducting experiments for all benchmarks (Table 2 in our paper) takes at least **20,160 hours** (24 hours * 14 benchmarks * 20 trials * 3 fuzzers + N hours for building benchmarks/fuzzers) on a single core.
Yet, we provide running script files which reproduce all result tables(Table 2, 3, 4, and 5) in our paper.


Once the setup is properly prepared (or using VM), enter the following command to reproduce the main evlauation results in our paper, which is Table 2 in Section IV.

## With Reproducing-Script Files

### Table 2 & Figure 3

`main_eval.sh` will sequentially run FuzzBench with the given configurations and perform 3 script files: make_local_config.sh, extract_raw_data.sh, and generate_table.py.

```
./main_eval.sh -t [TRIALS] -s [TIME] -p [EXP_PATH] -r [REPORT] -e [EXP_NAME]

 [TRIALS]: the number of trials of the intended experiments. The default value is 20
 [TIME]: the seconds for the running experiments. The default value is 86400, which is 24 hours
 [EXP_PATH]: the path for storing the running data of FuzzBench. The default path is [the working directory]/data/
 [REPORT]: the path for storing the report of FuzzBench. The default path is [the working directory]/report/
 [EXP_NAME]: the name of the experiments which will be stored in [EXP_PATH]. The default name is maintable
```

The below description is for summarizing the goal of each script file.

```
make_local_config.sh: a script file to generate a local experiment configuration file(local-experiment-config.yaml) which is mandatory for running FuzzBench on a local machine.
extract_raw_data.sh: a script file which extracts the information of crash inputs generated by each fuzzer.
generate_table.py: a python file which generates the result table as in our paper.
```



Note that running FuzzBench requires lots of memory and storage. It may not work properly when running `main_eval.sh` script with the default settings. 
For example, running `main_eval.sh` without any additional options requires at least 840 (20 trials * 14 benchmarks * 3 fuzzers) CPU power. (The version we used FuzzBench assigns 1 CPU power to each experiment on a local machine)
If FuzzBench is eventually dead with `EOF` error, please set [TRIALS] option to a value less than 20 to reduce memory and storage usage. 

### Table N

```
./running_table[N].sh -t [TRIALS] -s [TIME] -p [EXP_PATH] -r [REPORT] -e [EXP_NAME]
```

`running_table[N].sh` will make a local experiment configuration file(`local-experiment-config.yaml`) for running FuzzBench on the local machine, running FuzzBench with the benchmark programs and fuzzers used in Table [N] in our paper, and generate result table for them.
For example, running_table3.sh will evaluate the experiments used to the Table 3 in our paper.

The options for `running_table[N].sh` is the same as the ones for `main_eval.sh`

### Result
After running each script file, `result` directory is produced in the current directory.
The details are as follows:

```
result
└── coverage.csv : contains average coverage of each fuzzer on each benchmark. (follows FuzzBench criteria)
|      
├── crashes.csv  : contains the number of crash inputs generated by each fuzzer on each benchmark (follows AFL++ criteria)
|      
├── result_table : contains a table which integrates coverage.csv and crashes.csv
|
└── unique_vul_venn_diagram.png : A Venn diagram for the unique vulnerabilities found by each tool (follows FuzzBench criteria)
```

## Without Reproducing-Script file 
You may want to run FuzzBench with your own flavors in usage.

```
./make_local_config.sh $TRIALS $TIME $EXP_PATH $REPORT
cp ./local-experiment-config.yaml ./fuzzbench/

PYTHONPATH=./fuzzbench/ \
python3.9 ./fuzzbench/experiment/run_experiment.py -cb [N] -a -c ./fuzzbench/local-experiment-config.yaml \
-b [BENCHMARKS] -f [FUZZERS] -e [EXP_NAME]
```

```
-cb [N]: cuncurrent building options. Higher the value, faster the building process. 
[BENCHMARKS] : give the full name of the the programs which will be used for experiments. The benchmark programs are in ./fuzzbench/benchmarks directory. ex) "-b arrow_parquet-arrow-fuzz grok_grk_decompress_fuzzer"
[FUZZERS] : give the full name of the fuzzers which will be used for experiments. The fuzzers are in ./fuzzbench/fuzzers directory. ex) "-f aflpp aflppmopt seamfuzz"
```

# Contact
Myungho Lee (e-mail: myungho_lee@korea.ac.kr)

